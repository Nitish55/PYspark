{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* timestamp()\n",
    "* to_date()\n",
    "* date_format()\n",
    "* months_between()\n",
    "* explode nested array into rows\n",
    "* collect_list()\n",
    "* collect_set()\n",
    "* countDistinct()\n",
    "* map_keys() / Value()\n",
    "* struct()\n",
    "* rowNumber() Window function\n",
    "* rank Window Function\n",
    "* percent_rank Window Function\n",
    "* lag Window Function\n",
    "* lead Window Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.types import StructField, StructType, StringType, MapType\n",
    "from pyspark.sql.functions import col,lit,expr,explode,when,concat,split,substring,translate,to_timestamp\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName(\"Train_Hard\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|     input_timestamp|\n",
      "+---+--------------------+\n",
      "|  1|2019-06-24 12:01:...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.createDataFrame(\n",
    "        data = [ (\"1\",\"2019-06-24 12:01:19.000\")],\n",
    "        schema=[\"id\",\"input_timestamp\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting to timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------------------+\n",
      "| id|     input_timestamp|               time|\n",
      "+---+--------------------+-------------------+\n",
      "|  1|2019-06-24 12:01:...|2019-06-24 12:01:19|\n",
      "+---+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"time\",to_timestamp(\"input_timestamp\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "#### - Spark TimestampType format 'yyyy-MM-dd  HH:mm:ss.SSS'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that when dates are not in Spark Tiemstamp format, all Spark functions returns null\n",
    "- Hence, first convert the input dates to Spark DateType using to_timestamp function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|                 DT|\n",
      "+-------------------+\n",
      "|2019-06-24 12:01:19|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(to_timestamp(lit('06-24-2019 12:01:19.000'),'MM-dd-yyyy HH:mm:ss.SSSS').alias(\"DT\")) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|  DT|\n",
      "+----+\n",
      "|null|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(to_timestamp(lit('06-24-2019 12:01:19.000'),'dd-MM-yyyy HH:mm:ss.SSSS').alias(\"DT\")) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to_date() – \n",
    "* Convert Timestamp to Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|     input_timestamp|\n",
      "+---+--------------------+\n",
      "|  1|2019-06-24 12:01:...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.createDataFrame(\n",
    "        data = [ (\"1\",\"2019-06-24 12:01:19.000\")],\n",
    "        schema=[\"id\",\"input_timestamp\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting time string to date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+----------+\n",
      "|id |input_timestamp        |date_type |\n",
      "+---+-----------------------+----------+\n",
      "|1  |2019-06-24 12:01:19.000|2019-06-24|\n",
      "+---+-----------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"date_type\",to_date(\"input_timestamp\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fetching current date and timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+----------+\n",
      "|id |input_timestamp        |Curr_date |\n",
      "+---+-----------------------+----------+\n",
      "|1  |2019-06-24 12:01:19.000|2022-10-31|\n",
      "+---+-----------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"Curr_date\",current_date()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+--------------------------+\n",
      "|id |input_timestamp        |Curr_timestamp            |\n",
      "+---+-----------------------+--------------------------+\n",
      "|1  |2019-06-24 12:01:19.000|2022-10-31 00:31:02.024465|\n",
      "+---+-----------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"Curr_timestamp\",current_timestamp()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert TimestampType (timestamp) to DateType (date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+----------+-------------------+\n",
      "|id |input_timestamp        |Curr_date |timestamp          |\n",
      "+---+-----------------------+----------+-------------------+\n",
      "|1  |2019-06-24 12:01:19.000|2022-10-31|2019-06-24 12:01:19|\n",
      "+---+-----------------------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"Curr_date\",current_date()) \\\n",
    "  .withColumn(\"timestamp\",to_timestamp(\"input_timestamp\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is another way to convert TimestampType (timestamp string) to DateType using cast function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+----------+\n",
      "|id |input_timestamp        |date_type |\n",
      "+---+-----------------------+----------+\n",
      "|1  |2019-06-24 12:01:19.000|2019-06-24|\n",
      "+---+-----------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using Cast to convert Timestamp String to DateType\n",
    "df.withColumn('date_type', col('input_timestamp').cast('date')) \\\n",
    "       .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+----------+\n",
      "|id |input_timestamp        |date_type |\n",
      "+---+-----------------------+----------+\n",
      "|1  |2019-06-24 12:01:19.000|2019-06-24|\n",
      "+---+-----------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using Cast to convert TimestampType to DateType\n",
    "df.withColumn('date_type', to_timestamp('input_timestamp').cast('date')) \\\n",
    "  .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# date_format() \n",
    "- Convert Date to String format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.createDataFrame([[\"1\"]],[\"id\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+----------------+\n",
      "| id|Today's Date|yr-mnth-day|      MM/dd/yyyy|\n",
      "+---+------------+-----------+----------------+\n",
      "|  1|  2022-10-31| 2022-10-31|10/31/2022 01:38|\n",
      "+---+------------+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"*\"),current_date().alias(\"Today's Date\"), \\\n",
    "         date_format(current_timestamp(),\"yyyy-MM-dd\").alias(\"yr-mnth-day\"), \\\n",
    "         date_format(current_timestamp(),\"MM/dd/yyyy hh:mm\").alias(\"MM/dd/yyyy\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# datediff()\n",
    "* difference between two dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|      date|\n",
      "+---+----------+\n",
      "|  1|2019-07-01|\n",
      "|  2|2019-06-24|\n",
      "|  3|2019-08-24|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"1\",\"2019-07-01\"),(\"2\",\"2019-06-24\"),(\"3\",\"2019-08-24\")]\n",
    "df=spark.createDataFrame(data=data,schema=[\"id\",\"date\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------+---------+\n",
      "| id|      date|Current_Date|Date_Diff|\n",
      "+---+----------+------------+---------+\n",
      "|  1|2019-07-01|  2022-10-31|     1218|\n",
      "|  2|2019-06-24|  2022-10-31|     1225|\n",
      "|  3|2019-08-24|  2022-10-31|     1164|\n",
      "+---+----------+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"*\"),current_date().alias(\"Current_Date\"),datediff(current_date(),col(\"date\")).alias(\"Date_Diff\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# months_between()\n",
    "* to get month and year differences between two dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------+-----------+----------------+---------------+\n",
      "| id|      date|Current_Date| Month_Diff|Round_Month_Diff|Round_Year_Diff|\n",
      "+---+----------+------------+-----------+----------------+---------------+\n",
      "|  1|2019-07-01|  2022-10-31|39.96774194|           39.97|            3.0|\n",
      "|  2|2019-06-24|  2022-10-31|40.22580645|           40.23|            3.0|\n",
      "|  3|2019-08-24|  2022-10-31|38.22580645|           38.23|            3.0|\n",
      "+---+----------+------------+-----------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"*\"),current_date().alias(\"Current_Date\"),\\\n",
    "          months_between(current_date(),col(\"date\")).alias(\"Month_Diff\"),\n",
    "          round(months_between(current_date(),col(\"date\")),2).alias(\"Round_Month_Diff\"),\\\n",
    "          round(months_between(current_date(),col(\"date\"))/lit(12)).alias(\"Round_Year_Diff\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark – explode nested array into rows\n",
    "* Problem: How to explode & flatten nested array (Array of Array) DataFrame columns into rows using PySpark.\n",
    "\n",
    "* Solution: PySpark explode function can be used to explode an Array of Array (nested Array) ArrayType(ArrayType(StringType)) columns to rows on PySpark DataFrame using python example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------------------+\n",
      "|name   |subjects                           |\n",
      "+-------+-----------------------------------+\n",
      "|James  |[[Java, Scala, C++], [Spark, Java]]|\n",
      "|Michael|[[Spark, Java, C++], [Spark, Java]]|\n",
      "|Robert |[[CSharp, VB], [Spark, Python]]    |\n",
      "+-------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "arrayArrayData = [\n",
    "  (\"James\",[[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"]]),\n",
    "  (\"Michael\",[[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"]]),\n",
    "  (\"Robert\",[[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"]])\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=arrayArrayData, schema = ['name','subjects'])\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|name   |col               |\n",
      "+-------+------------------+\n",
      "|James  |[Java, Scala, C++]|\n",
      "|James  |[Spark, Java]     |\n",
      "|Michael|[Spark, Java, C++]|\n",
      "|Michael|[Spark, Java]     |\n",
      "|Robert |[CSharp, VB]      |\n",
      "|Robert |[Spark, Python]   |\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.name,explode(df.subjects)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you want to flatten the arrays, use flatten function which converts array of array columns to a single array on DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------------------+\n",
      "|name   |flatten(subjects)              |\n",
      "+-------+-------------------------------+\n",
      "|James  |[Java, Scala, C++, Spark, Java]|\n",
      "|Michael|[Spark, Java, C++, Spark, Java]|\n",
      "|Robert |[CSharp, VB, Spark, Python]    |\n",
      "+-------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.name,flatten(df.subjects)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# array_contains() \n",
    "* sql function is used to check if array column contains a value. Returns null if the array is null, true if the array contains the value, and false otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+---------------+------------+-------------+\n",
      "|            name| languagesAtSchool|languagesAtWork|currentState|previousState|\n",
      "+----------------+------------------+---------------+------------+-------------+\n",
      "|    James,,Smith|[Java, Scala, C++]|  [Spark, Java]|          OH|           CA|\n",
      "|   Michael,Rose,|[Spark, Java, C++]|  [Spark, Java]|          NY|           NJ|\n",
      "|Robert,,Williams|      [CSharp, VB]|[Spark, Python]|          UT|           NV|\n",
      "+----------------+------------------+---------------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = [\n",
    " (\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"],\"OH\",\"CA\"),\n",
    " (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"],\"NY\",\"NJ\"),\n",
    " (\"Robert,,Williams\",[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"],\"UT\",\"NV\")\n",
    "]\n",
    "\n",
    "from pyspark.sql.types import StringType, ArrayType,StructType,StructField\n",
    "schema = StructType([ \n",
    "    StructField(\"name\",StringType(),True), \n",
    "    StructField(\"languagesAtSchool\",ArrayType(StringType()),True), \n",
    "    StructField(\"languagesAtWork\",ArrayType(StringType()),True), \n",
    "    StructField(\"currentState\", StringType(), True), \n",
    "    StructField(\"previousState\", StringType(), True)\n",
    "  ])\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+---------------+------------+-------------+--------------+\n",
      "|            name| languagesAtSchool|languagesAtWork|currentState|previousState|array_contains|\n",
      "+----------------+------------------+---------------+------------+-------------+--------------+\n",
      "|    James,,Smith|[Java, Scala, C++]|  [Spark, Java]|          OH|           CA|          true|\n",
      "|   Michael,Rose,|[Spark, Java, C++]|  [Spark, Java]|          NY|           NJ|          true|\n",
      "|Robert,,Williams|      [CSharp, VB]|[Spark, Python]|          UT|           NV|         false|\n",
      "+----------------+------------------+---------------+------------+-------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"*\"),array_contains(col(\"languagesAtSchool\"),\"Java\").alias(\"array_contains\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# array() \n",
    "* function to create a new array column by merging the data from multiple columns. All input columns must have the same data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|Combine                            |\n",
      "+-----------------------------------+\n",
      "|[[Java, Scala, C++], [Spark, Java]]|\n",
      "|[[Spark, Java, C++], [Spark, Java]]|\n",
      "|[[CSharp, VB], [Spark, Python]]    |\n",
      "+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(array(df.languagesAtSchool,df.languagesAtWork).alias(\"Combine\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# collect_list() \n",
    "* function returns all values from an input column with duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|James        |Sales     |3000  |\n",
      "|Michael      |Sales     |4600  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|James        |Sales     |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "simpleData = [(\"James\", \"Sales\", 3000),\n",
    "    (\"Michael\", \"Sales\", 4600),\n",
    "    (\"Robert\", \"Sales\", 4100),\n",
    "    (\"Maria\", \"Finance\", 3000),\n",
    "    (\"James\", \"Sales\", 3000),\n",
    "    (\"Scott\", \"Finance\", 3300),\n",
    "    (\"Jen\", \"Finance\", 3900),\n",
    "    (\"Jeff\", \"Marketing\", 3000),\n",
    "    (\"Kumar\", \"Marketing\", 2000),\n",
    "    (\"Saif\", \"Sales\", 4100)\n",
    "  ]\n",
    "schema = [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------+\n",
      "|collect_list(salary)                                        |\n",
      "+------------------------------------------------------------+\n",
      "|[3000, 4600, 4100, 3000, 3000, 3300, 3900, 3000, 2000, 4100]|\n",
      "+------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(collect_list(\"salary\")).show(truncate=False)\n",
    "\n",
    "# gives output in the form of a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|salary|\n",
      "+------+\n",
      "|  3000|\n",
      "|  4600|\n",
      "|  4100|\n",
      "|  3000|\n",
      "|  3000|\n",
      "|  3300|\n",
      "|  3900|\n",
      "|  3000|\n",
      "|  2000|\n",
      "|  4100|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"salary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# collect_set() \n",
    "* function returns all values from an input column with duplicate values eliminated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n",
      "|collect_set(salary)                 |\n",
      "+------------------------------------+\n",
      "|[4600, 3000, 3900, 4100, 3300, 2000]|\n",
      "+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(collect_set(\"salary\")).show(truncate=False)\n",
    "\n",
    "# gives output in the form of a list with duplicates removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# countDistinct() \n",
    "* function returns the number of distinct elements in a columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|count(DISTINCT department, salary)|\n",
      "+----------------------------------+\n",
      "|8                                 |\n",
      "+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.select(countDistinct(\"department\", \"salary\"))\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct Count of Department & Salary: Row(count(DISTINCT department, salary)=8)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Distinct Count of Department & Salary: \"+str(df2.collect()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct Count of Department & Salary: 8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Distinct Count of Department & Salary: \"+str(df2.collect()[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# first() \n",
    "* function returns the first element in a column when ignoreNulls is set to true, it returns the first non-null element.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|first(salary)|\n",
      "+-------------+\n",
      "|         3000|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(first(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# last()\n",
    "* when ignoreNulls is set to true, it returns the last non-null element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|last(salary)|\n",
      "+------------+\n",
      "|        4100|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(last(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create_map() \n",
    "* is used to convert selected DataFrame columns to MapType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+------+--------+\n",
      "|id   |dept     |salary|location|\n",
      "+-----+---------+------+--------+\n",
      "|36636|Finance  |3000  |USA     |\n",
      "|40288|Finance  |5000  |IND     |\n",
      "|42114|Sales    |3900  |USA     |\n",
      "|39192|Marketing|2500  |CAN     |\n",
      "|34534|Sales    |6500  |USA     |\n",
      "+-----+---------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = [ (\"36636\",\"Finance\",3000,\"USA\"), \n",
    "    (\"40288\",\"Finance\",5000,\"IND\"), \n",
    "    (\"42114\",\"Sales\",3900,\"USA\"), \n",
    "    (\"39192\",\"Marketing\",2500,\"CAN\"), \n",
    "    (\"34534\",\"Sales\",6500,\"USA\") ]\n",
    "schema = StructType([\n",
    "     StructField('id', StringType(), True),\n",
    "     StructField('dept', StringType(), True),\n",
    "     StructField('salary', IntegerType(), True),\n",
    "     StructField('location', StringType(), True)\n",
    "     ])\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+------+--------+---------------------------------+\n",
      "|id   |dept     |salary|location|propertiesMap                    |\n",
      "+-----+---------+------+--------+---------------------------------+\n",
      "|36636|Finance  |3000  |USA     |{salary -> 3000, Location -> USA}|\n",
      "|40288|Finance  |5000  |IND     |{salary -> 5000, Location -> IND}|\n",
      "|42114|Sales    |3900  |USA     |{salary -> 3900, Location -> USA}|\n",
      "|39192|Marketing|2500  |CAN     |{salary -> 2500, Location -> CAN}|\n",
      "|34534|Sales    |6500  |USA     |{salary -> 6500, Location -> USA}|\n",
      "+-----+---------+------+--------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"propertiesMap\",create_map(lit(\"salary\"),col(\"salary\"),\n",
    "                                        lit(\"Location\"),col(\"location\"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---------------------------------+\n",
      "|id   |dept     |propertiesMap                    |\n",
      "+-----+---------+---------------------------------+\n",
      "|36636|Finance  |{salary -> 3000, Location -> USA}|\n",
      "|40288|Finance  |{salary -> 5000, Location -> IND}|\n",
      "|42114|Sales    |{salary -> 3900, Location -> USA}|\n",
      "|39192|Marketing|{salary -> 2500, Location -> CAN}|\n",
      "|34534|Sales    |{salary -> 6500, Location -> USA}|\n",
      "+-----+---------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"propertiesMap\",create_map(lit(\"salary\"),col(\"salary\"),\n",
    "                                        lit(\"Location\"),col(\"location\"))).drop(\"location\",\"salary\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# map_keys() – \n",
    "* Get All Map Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------------+\n",
      "|Name      |properties                   |\n",
      "+----------+-----------------------------+\n",
      "|James     |{eye -> brown, hair -> black}|\n",
      "|Michael   |{eye -> null, hair -> brown} |\n",
      "|Robert    |{eye -> black, hair -> red}  |\n",
      "|Washington|{eye -> grey, hair -> grey}  |\n",
      "|Jefferson |{eye -> , hair -> brown}     |\n",
      "+----------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataDictionary = [\n",
    "        ('James',{'hair':'black','eye':'brown'}),\n",
    "        ('Michael',{'hair':'brown','eye':None}),\n",
    "        ('Robert',{'hair':'red','eye':'black'}),\n",
    "        ('Washington',{'hair':'grey','eye':'grey'}),\n",
    "        ('Jefferson',{'hair':'brown','eye':''})\n",
    "        ]\n",
    "df = spark.createDataFrame(data=dataDictionary, schema = [\"Name\",\"properties\"])\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|      Name|map_keys(properties)|\n",
      "+----------+--------------------+\n",
      "|     James|         [eye, hair]|\n",
      "|   Michael|         [eye, hair]|\n",
      "|    Robert|         [eye, hair]|\n",
      "|Washington|         [eye, hair]|\n",
      "| Jefferson|         [eye, hair]|\n",
      "+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.Name,map_keys(df.properties)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map_Values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------------+\n",
      "|      Name|map_values(properties)|\n",
      "+----------+----------------------+\n",
      "|     James|        [brown, black]|\n",
      "|   Michael|         [null, brown]|\n",
      "|    Robert|          [black, red]|\n",
      "|Washington|          [grey, grey]|\n",
      "| Jefferson|             [, brown]|\n",
      "+----------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.Name,map_values(df.properties)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Struct()\n",
    "* we can change the struct of the existing DataFrame and add a new StructType to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+------+\n",
      "|name                |id   |gender|salary|\n",
      "+--------------------+-----+------+------+\n",
      "|{James, , Smith}    |36636|M     |3100  |\n",
      "|{Michael, Rose, }   |40288|M     |4300  |\n",
      "|{Robert, , Williams}|42114|M     |1400  |\n",
      "|{Maria, Anne, Jones}|39192|F     |5500  |\n",
      "|{Jen, Mary, Brown}  |     |F     |-1    |\n",
      "+--------------------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "structureData = [\n",
    "    ((\"James\",\"\",\"Smith\"),\"36636\",\"M\",3100),\n",
    "    ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",4300),\n",
    "    ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",1400),\n",
    "    ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",5500),\n",
    "    ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",-1)\n",
    "  ]\n",
    "structureSchema = StructType([\n",
    "        StructField('name', StructType([\n",
    "             StructField('firstname', StringType(), True),\n",
    "             StructField('middlename', StringType(), True),\n",
    "             StructField('lastname', StringType(), True)\n",
    "             ])),\n",
    "         StructField('id', StringType(), True),\n",
    "         StructField('gender', StringType(), True),\n",
    "         StructField('salary', IntegerType(), True)\n",
    "         ])\n",
    "\n",
    "df2 = spark.createDataFrame(data=structureData,schema=structureSchema)\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------------+\n",
      "|name                |OtherInfo               |\n",
      "+--------------------+------------------------+\n",
      "|{James, , Smith}    |{36636, M, 3100, MEDIUM}|\n",
      "|{Michael, Rose, }   |{40288, M, 4300, HIGH}  |\n",
      "|{Robert, , Williams}|{42114, M, 1400, LOW}   |\n",
      "|{Maria, Anne, Jones}|{39192, F, 5500, HIGH}  |\n",
      "|{Jen, Mary, Brown}  |{, F, -1, LOW}          |\n",
      "+--------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "updatedDF = df2.withColumn(\"OtherInfo\",\n",
    "           struct(col(\"ID\").alias(\"Identity\"),\n",
    "                 col(\"gender\").alias(\"SEX\"),\n",
    "                  col(\"salary\").alias(\"Kamai\"),\n",
    "                 when(col(\"salary\").cast(IntegerType())<2000,\"LOW\")\n",
    "                  .when(col(\"salary\").cast(IntegerType())< 4000,\"MEDIUM\")\n",
    "                  .otherwise(\"HIGH\").alias(\"Grade\"))).drop(\"id\",\"gender\",\"salary\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## row_number Window Function\n",
    "* row_number() window function is used to give the sequential row number starting from 1 to the result of each window partition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|James        |Sales     |3000  |\n",
      "|Michael      |Sales     |4600  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|James        |Sales     |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = ((\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600),  \\\n",
    "    (\"Robert\", \"Sales\", 4100),   \\\n",
    "    (\"Maria\", \"Finance\", 3000),  \\\n",
    "    (\"James\", \"Sales\", 3000),    \\\n",
    "    (\"Scott\", \"Finance\", 3300),  \\\n",
    "    (\"Jen\", \"Finance\", 3900),    \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000),\\\n",
    "    (\"Saif\", \"Sales\", 4100) \\\n",
    "  )\n",
    " \n",
    "columns= [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|row_number|\n",
      "+-------------+----------+------+----------+\n",
      "|Maria        |Finance   |3000  |1         |\n",
      "|Scott        |Finance   |3300  |2         |\n",
      "|Jen          |Finance   |3900  |3         |\n",
      "|Kumar        |Marketing |2000  |1         |\n",
      "|Jeff         |Marketing |3000  |2         |\n",
      "|James        |Sales     |3000  |1         |\n",
      "|James        |Sales     |3000  |2         |\n",
      "|Robert       |Sales     |4100  |3         |\n",
      "|Saif         |Sales     |4100  |4         |\n",
      "|Michael      |Sales     |4600  |5         |\n",
      "+-------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "ws=Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "df.withColumn(\"row_number\",row_number().over(ws)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rank Window Function\n",
    "* rank() window function is used to provide a rank to the result within a window partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|rank|\n",
      "+-------------+----------+------+----+\n",
      "|        Maria|   Finance|  3000|   1|\n",
      "|        Scott|   Finance|  3300|   2|\n",
      "|          Jen|   Finance|  3900|   3|\n",
      "|        Kumar| Marketing|  2000|   1|\n",
      "|         Jeff| Marketing|  3000|   2|\n",
      "|        James|     Sales|  3000|   1|\n",
      "|        James|     Sales|  3000|   1|\n",
      "|       Robert|     Sales|  4100|   3|\n",
      "|         Saif|     Sales|  4100|   3|\n",
      "|      Michael|     Sales|  4600|   5|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"rank\",rank().over(ws)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dense_rank Window Function\n",
    "* dense_rank() window function is used to get the result with rank of rows within a window partition without any gaps. This is similar to rank() function difference being rank function leaves gaps in rank when there are ties.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|rank|\n",
      "+-------------+----------+------+----+\n",
      "|        Maria|   Finance|  3000|   1|\n",
      "|        Scott|   Finance|  3300|   2|\n",
      "|          Jen|   Finance|  3900|   3|\n",
      "|        Kumar| Marketing|  2000|   1|\n",
      "|         Jeff| Marketing|  3000|   2|\n",
      "|        James|     Sales|  3000|   1|\n",
      "|        James|     Sales|  3000|   1|\n",
      "|       Robert|     Sales|  4100|   2|\n",
      "|         Saif|     Sales|  4100|   2|\n",
      "|      Michael|     Sales|  4600|   3|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"rank\",dense_rank().over(ws)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## percent_rank Window Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+------------+\n",
      "|employee_name|department|salary|percent_rank|\n",
      "+-------------+----------+------+------------+\n",
      "|        Maria|   Finance|  3000|         0.0|\n",
      "|        Scott|   Finance|  3300|         0.5|\n",
      "|          Jen|   Finance|  3900|         1.0|\n",
      "|        Kumar| Marketing|  2000|         0.0|\n",
      "|         Jeff| Marketing|  3000|         1.0|\n",
      "|        James|     Sales|  3000|         0.0|\n",
      "|        James|     Sales|  3000|         0.0|\n",
      "|       Robert|     Sales|  4100|         0.5|\n",
      "|         Saif|     Sales|  4100|         0.5|\n",
      "|      Michael|     Sales|  4600|         1.0|\n",
      "+-------------+----------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"percent_rank\",percent_rank().over(ws)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ntile Window Function\n",
    "* ntile() window function returns the relative rank of result rows within a window partition. In below example we have used 2 as an argument to ntile hence it returns ranking between 2 values (1 and 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+-----+\n",
      "|employee_name|department|salary|ntile|\n",
      "+-------------+----------+------+-----+\n",
      "|        Maria|   Finance|  3000|    1|\n",
      "|        Scott|   Finance|  3300|    1|\n",
      "|          Jen|   Finance|  3900|    2|\n",
      "|        Kumar| Marketing|  2000|    1|\n",
      "|         Jeff| Marketing|  3000|    2|\n",
      "|        James|     Sales|  3000|    1|\n",
      "|        James|     Sales|  3000|    1|\n",
      "|       Robert|     Sales|  4100|    1|\n",
      "|         Saif|     Sales|  4100|    2|\n",
      "|      Michael|     Sales|  4600|    2|\n",
      "+-------------+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"ntile\",ntile(2).over(ws)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+-----+\n",
      "|employee_name|department|salary|ntile|\n",
      "+-------------+----------+------+-----+\n",
      "|        Maria|   Finance|  3000|    1|\n",
      "|        Scott|   Finance|  3300|    2|\n",
      "|          Jen|   Finance|  3900|    3|\n",
      "|        Kumar| Marketing|  2000|    1|\n",
      "|         Jeff| Marketing|  3000|    2|\n",
      "|        James|     Sales|  3000|    1|\n",
      "|        James|     Sales|  3000|    2|\n",
      "|       Robert|     Sales|  4100|    3|\n",
      "|         Saif|     Sales|  4100|    4|\n",
      "|      Michael|     Sales|  4600|    5|\n",
      "+-------------+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"ntile\",ntile(30).over(ws)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lag Window Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary| lag|\n",
      "+-------------+----------+------+----+\n",
      "|        Maria|   Finance|  3000|null|\n",
      "|        Scott|   Finance|  3300|null|\n",
      "|          Jen|   Finance|  3900|3000|\n",
      "|        Kumar| Marketing|  2000|null|\n",
      "|         Jeff| Marketing|  3000|null|\n",
      "|        James|     Sales|  3000|null|\n",
      "|        James|     Sales|  3000|null|\n",
      "|       Robert|     Sales|  4100|3000|\n",
      "|         Saif|     Sales|  4100|3000|\n",
      "|      Michael|     Sales|  4600|4100|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"lag\",lag(\"salary\",2).over(ws)) \\\n",
    "      .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lead Window Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|lead|\n",
      "+-------------+----------+------+----+\n",
      "|        Maria|   Finance|  3000|3900|\n",
      "|        Scott|   Finance|  3300|null|\n",
      "|          Jen|   Finance|  3900|null|\n",
      "|        Kumar| Marketing|  2000|null|\n",
      "|         Jeff| Marketing|  3000|null|\n",
      "|        James|     Sales|  3000|4100|\n",
      "|        James|     Sales|  3000|4100|\n",
      "|       Robert|     Sales|  4100|4600|\n",
      "|         Saif|     Sales|  4100|null|\n",
      "|      Michael|     Sales|  4600|null|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"lead\",lead(\"salary\",2).over(ws)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:SPARK_HOME]",
   "language": "python",
   "name": "conda-env-SPARK_HOME-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
