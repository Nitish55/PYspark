{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Drop\n",
    "* fill\n",
    "* Replace\n",
    "* Trunc\n",
    "* Explode--- explode_outer\n",
    "* Flatten\n",
    "* Format_string\n",
    "* JSON functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql.functions import udf,col,length,expr,lit,lower,sumDistinct,sum,round,count,countDistinct,max,min,avg\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"Train_hard\").getOrCreate()\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.window import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import MapType,StringType,StructField"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NA_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------+\n",
      "|  name| Age|Height|\n",
      "+------+----+------+\n",
      "| Alice|  80|    10|\n",
      "|   Bob|null|     5|\n",
      "|   Tom|  50|    50|\n",
      "|  null|null|  null|\n",
      "|Robert|  30|    35|\n",
      "|   Jin|null|  null|\n",
      "+------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(\"Alice\",80,10),(\"Bob\",None,5),(\"Tom\",50,50),(None,None,None),(\"Robert\",30,35),(\"Jin\",None,None)]\n",
    "schema=\"name string, Age int, Height int\"\n",
    "df=spark.createDataFrame(data=data,schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------+\n",
      "|  name|Age|Height|\n",
      "+------+---+------+\n",
      "| Alice| 80|    10|\n",
      "|   Tom| 50|    50|\n",
      "|Robert| 30|    35|\n",
      "+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.dropna().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How in Drop\n",
    "* by defaut its how=\"any\"\n",
    "* if we give how=\"all\", row having all null value will be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------+\n",
      "|  name| Age|Height|\n",
      "+------+----+------+\n",
      "| Alice|  80|    10|\n",
      "|   Bob|null|     5|\n",
      "|   Tom|  50|    50|\n",
      "|Robert|  30|    35|\n",
      "|   Jin|null|  null|\n",
      "+------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.dropna(how='all').show()\n",
    "# only row having all elemnt as null is dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thres\n",
    "* thres=1, means if having less than 1 not-null value, row 4 have all null value so it's dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------+\n",
      "|  name| Age|Height|\n",
      "+------+----+------+\n",
      "| Alice|  80|    10|\n",
      "|   Bob|null|     5|\n",
      "|   Tom|  50|    50|\n",
      "|Robert|  30|    35|\n",
      "|   Jin|null|  null|\n",
      "+------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.dropna(thresh=1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------+\n",
      "|  name| Age|Height|\n",
      "+------+----+------+\n",
      "| Alice|  80|    10|\n",
      "|   Bob|null|     5|\n",
      "|   Tom|  50|    50|\n",
      "|Robert|  30|    35|\n",
      "+------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.dropna(thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------+\n",
      "|  name|Age|Height|\n",
      "+------+---+------+\n",
      "| Alice| 80|    10|\n",
      "|   Tom| 50|    50|\n",
      "|Robert| 30|    35|\n",
      "+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.dropna(thresh=3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using-- Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------+\n",
      "|  name| Age|Height|\n",
      "+------+----+------+\n",
      "| Alice|  80|    10|\n",
      "|   Bob|null|     5|\n",
      "|   Tom|  50|    50|\n",
      "|Robert|  30|    35|\n",
      "+------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.dropna(subset=\"Height\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------+\n",
      "|  name|Age|Height|\n",
      "+------+---+------+\n",
      "| Alice| 80|    10|\n",
      "|   Tom| 50|    50|\n",
      "|Robert| 30|    35|\n",
      "+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.dropna(subset=[\"Age\",\"Height\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill\n",
    "* we pass 55 as int so it filled only int columns string column remain as it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------+\n",
      "|  name|Age|Height|\n",
      "+------+---+------+\n",
      "| Alice| 80|    10|\n",
      "|   Bob| 55|     5|\n",
      "|   Tom| 50|    50|\n",
      "|  null| 55|    55|\n",
      "|Robert| 30|    35|\n",
      "|   Jin| 55|    55|\n",
      "+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.fillna(55).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------+\n",
      "|  name| Age|Height|\n",
      "+------+----+------+\n",
      "| Alice|  80|    10|\n",
      "|   Bob|null|     5|\n",
      "|   Tom|  50|    50|\n",
      "|  king|null|  null|\n",
      "|Robert|  30|    35|\n",
      "|   Jin|null|  null|\n",
      "+------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.fillna(\"king\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another way using multiple variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------+\n",
      "|  name|Age|Height|\n",
      "+------+---+------+\n",
      "| Alice| 80|    10|\n",
      "|   Bob|121|     5|\n",
      "|   Tom| 50|    50|\n",
      "| Rocky|121|  null|\n",
      "|Robert| 30|    35|\n",
      "|   Jin|121|  null|\n",
      "+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.fillna({\"name\":\"Rocky\",\"Age\":121}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------+\n",
      "|  name| Age|Height|\n",
      "+------+----+------+\n",
      "| Alice|  80|    10|\n",
      "|   Bob|null|     5|\n",
      "|   Tom| 155|   155|\n",
      "|  null|null|  null|\n",
      "|Robert|  30|    35|\n",
      "|   Jin|null|  null|\n",
      "+------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.replace(50,155).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------+\n",
      "|  name| Age|Height|\n",
      "+------+----+------+\n",
      "| Alice|  80|    10|\n",
      "|   Bob|null|     5|\n",
      "|   Tom| 155|    50|\n",
      "|  null|null|  null|\n",
      "|Robert|  30|    35|\n",
      "|   Jin|null|  null|\n",
      "+------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.replace(50,155,subset=\"Age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------+\n",
      "|  name| Age|Height|\n",
      "+------+----+------+\n",
      "|  Alex|  80|    10|\n",
      "| Bobby|null|     5|\n",
      "|   Tom|  50|    50|\n",
      "|  null|null|  null|\n",
      "|Robert|  30|    35|\n",
      "|   Jin|null|  null|\n",
      "+------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.replace({\"Alice\":\"Alex\",\"Bob\":\"Bobby\"},subset=\"name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ABS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "|  -5|   0|\n",
      "|   1|   3|\n",
      "|   7|   9|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1=spark.createDataFrame([(-5,0),(1,3),(7,9)],(\"col1\",\"col2\"))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trunc--can be applied to timestamp or date column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|            11318|       COMPLETE|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.load('orders/part-00000',format=\"csv\",schema='order_id int,order_date timestamp,order_customer_id int, \\\n",
    "                   order_status string')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------------+\n",
      "|         order_date|trunc(order_date, year)|\n",
      "+-------------------+-----------------------+\n",
      "|2014-01-30 00:00:00|             2014-01-01|\n",
      "|2013-08-30 00:00:00|             2013-01-01|\n",
      "|2014-04-28 00:00:00|             2014-01-01|\n",
      "|2013-09-03 00:00:00|             2013-01-01|\n",
      "|2013-10-25 00:00:00|             2013-01-01|\n",
      "+-------------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"order_date\",trunc(df.order_date,\"year\")).distinct().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------------+\n",
      "|         order_date|trunc(order_date, mon)|\n",
      "+-------------------+----------------------+\n",
      "|2014-01-30 00:00:00|            2014-01-01|\n",
      "|2013-12-16 00:00:00|            2013-12-01|\n",
      "|2013-12-26 00:00:00|            2013-12-01|\n",
      "|2014-03-12 00:00:00|            2014-03-01|\n",
      "|2013-12-18 00:00:00|            2013-12-01|\n",
      "+-------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"order_date\",trunc(df.order_date,\"mon\")).distinct().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-----------------------------+\n",
      "|Empname|Languages    |Properties                   |\n",
      "+-------+-------------+-----------------------------+\n",
      "|Alicia |[Java, Scala]|{Eye -> Brown, Hair -> Black}|\n",
      "|Robert |[Spark, Java]|{Eye -> null, Hair -> Brown} |\n",
      "|Mike   |[CSharp, ]   |{Eye -> , Hair -> Red}       |\n",
      "|John   |null         |null                         |\n",
      "|Jeff   |[1, 2]       |{}                           |\n",
      "+-------+-------------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(\"Alicia\",[\"Java\",\"Scala\"],{\"Hair\":\"Black\",\"Eye\":\"Brown\"}),\\\n",
    "     (\"Robert\",[\"Spark\",\"Java\"],{\"Hair\":\"Brown\",\"Eye\":None}),\\\n",
    "     (\"Mike\",[\"CSharp\",\"\"],{\"Hair\":\"Red\",\"Eye\":\"\"}),\\\n",
    "     (\"John\",None,None),\\\n",
    "     (\"Jeff\",[\"1\",\"2\"],{})]\n",
    "schema=[\"Empname\",\"Languages\",\"Properties\"]\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|Empname|   col|\n",
      "+-------+------+\n",
      "| Alicia|  Java|\n",
      "| Alicia| Scala|\n",
      "| Robert| Spark|\n",
      "| Robert|  Java|\n",
      "|   Mike|CSharp|\n",
      "|   Mike|      |\n",
      "|   Jeff|     1|\n",
      "|   Jeff|     2|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.Empname,explode(df.Languages)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: HERE\n",
    "* John have null record in the array so its ignored in explode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explode on Map column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Empname: string (nullable = true)\n",
      " |-- Languages: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- Properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+\n",
      "|Empname| key|value|\n",
      "+-------+----+-----+\n",
      "| Alicia| Eye|Brown|\n",
      "| Alicia|Hair|Black|\n",
      "| Robert| Eye| null|\n",
      "| Robert|Hair|Brown|\n",
      "|   Mike| Eye|     |\n",
      "|   Mike|Hair|  Red|\n",
      "+-------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.Empname,explode(df.Properties)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explode_Outer-- \n",
    "* Unlike explode it returns null values as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|Empname|   col|\n",
      "+-------+------+\n",
      "| Alicia|  Java|\n",
      "| Alicia| Scala|\n",
      "| Robert| Spark|\n",
      "| Robert|  Java|\n",
      "|   Mike|CSharp|\n",
      "|   Mike|      |\n",
      "|   John|  null|\n",
      "|   Jeff|     1|\n",
      "|   Jeff|     2|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.Empname,explode_outer(df.Languages)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+\n",
      "|Empname| key|value|\n",
      "+-------+----+-----+\n",
      "| Alicia| Eye|Brown|\n",
      "| Alicia|Hair|Black|\n",
      "| Robert| Eye| null|\n",
      "| Robert|Hair|Brown|\n",
      "|   Mike| Eye|     |\n",
      "|   Mike|Hair|  Red|\n",
      "|   John|null| null|\n",
      "|   Jeff|null| null|\n",
      "+-------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.Empname,explode_outer(df.Properties)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posexplode\n",
    "* Display position as well, IGnores null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+\n",
      "|Empname|pos|   col|\n",
      "+-------+---+------+\n",
      "| Alicia|  0|  Java|\n",
      "| Alicia|  1| Scala|\n",
      "| Robert|  0| Spark|\n",
      "| Robert|  1|  Java|\n",
      "|   Mike|  0|CSharp|\n",
      "|   Mike|  1|      |\n",
      "|   Jeff|  0|     1|\n",
      "|   Jeff|  1|     2|\n",
      "+-------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.Empname,posexplode(df.Languages)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----+-----+\n",
      "|Empname|pos| key|value|\n",
      "+-------+---+----+-----+\n",
      "| Alicia|  0| Eye|Brown|\n",
      "| Alicia|  1|Hair|Black|\n",
      "| Robert|  0| Eye| null|\n",
      "| Robert|  1|Hair|Brown|\n",
      "|   Mike|  0| Eye|     |\n",
      "|   Mike|  1|Hair|  Red|\n",
      "+-------+---+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.Empname,posexplode(df.Properties)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## posexplode_outer\n",
    "* Reports NUll as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+\n",
      "|Empname| pos|   col|\n",
      "+-------+----+------+\n",
      "| Alicia|   0|  Java|\n",
      "| Alicia|   1| Scala|\n",
      "| Robert|   0| Spark|\n",
      "| Robert|   1|  Java|\n",
      "|   Mike|   0|CSharp|\n",
      "|   Mike|   1|      |\n",
      "|   John|null|  null|\n",
      "|   Jeff|   0|     1|\n",
      "|   Jeff|   1|     2|\n",
      "+-------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.Empname,posexplode_outer(df.Languages)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----+-----+\n",
      "|Empname| pos| key|value|\n",
      "+-------+----+----+-----+\n",
      "| Alicia|   0| Eye|Brown|\n",
      "| Alicia|   1|Hair|Black|\n",
      "| Robert|   0| Eye| null|\n",
      "| Robert|   1|Hair|Brown|\n",
      "|   Mike|   0| Eye|     |\n",
      "|   Mike|   1|Hair|  Red|\n",
      "|   John|null|null| null|\n",
      "|   Jeff|null|null| null|\n",
      "+-------+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.Empname,posexplode_outer(df.Properties)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten--Convert an array of array into a single Array column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------------------+\n",
      "|Empname|ArrayOfArrays              |\n",
      "+-------+---------------------------+\n",
      "|Alicia |[[Java], [Scala], [Python]]|\n",
      "|Robert |[[null], [Java], [Hadoop]] |\n",
      "+-------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(\"Alicia\",[[\"Java\"],[\"Scala\"],[\"Python\"]]),\\\n",
    "     (\"Robert\",[[None],[\"Java\"],[\"Hadoop\"]])]\n",
    "schema=[\"Empname\",\"ArrayOfArrays\"]\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------------------+----------------------+\n",
      "|Empname|ArrayOfArrays              |flatten(ArrayOfArrays)|\n",
      "+-------+---------------------------+----------------------+\n",
      "|Alicia |[[Java], [Scala], [Python]]|[Java, Scala, Python] |\n",
      "|Robert |[[null], [Java], [Hadoop]] |[null, Java, Hadoop]  |\n",
      "+-------+---------------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"*\"),flatten(df.ArrayOfArrays)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faltten when we pass not a nested list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+\n",
      "|Empname|ArrayOfArrays|\n",
      "+-------+-------------+\n",
      "|Alicia |[[1], [2]]   |\n",
      "|Robert |[null, [55]] |\n",
      "+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(\"Alicia\",[[1],[2]]),\\\n",
    "     (\"Robert\",[None,[55]])]\n",
    "schema=[\"Empname\",\"ArrayOfArrays\"]\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+----------------------+\n",
      "|Empname|ArrayOfArrays|flatten(ArrayOfArrays)|\n",
      "+-------+-------------+----------------------+\n",
      "|Alicia |[[1], [2]]   |[1, 2]                |\n",
      "|Robert |[null, [55]] |null                  |\n",
      "+-------+-------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"*\"),flatten(df.ArrayOfArrays)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### as in case of robert-\n",
    "* There is no nested list so null is returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+\n",
      "|Empname|ArrayOfArrays |\n",
      "+-------+--------------+\n",
      "|Alicia |[[1], [2]]    |\n",
      "|Robert |[[null], [55]]|\n",
      "+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(\"Alicia\",[[1],[2]]),\\\n",
    "     (\"Robert\",[[None],[55]])]\n",
    "schema=[\"Empname\",\"ArrayOfArrays\"]\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+----------------------+\n",
      "|Empname|ArrayOfArrays |flatten(ArrayOfArrays)|\n",
      "+-------+--------------+----------------------+\n",
      "|Alicia |[[1], [2]]    |[1, 2]                |\n",
      "|Robert |[[null], [55]]|[null, 55]            |\n",
      "+-------+--------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"*\"),flatten(df.ArrayOfArrays)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note\n",
    "* we can apply multiple flatten functions as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------------+----------------------+\n",
      "| ArrayOfArrays|flatten(ArrayOfArrays)|flatten(ArrayOfArrays)|\n",
      "+--------------+----------------------+----------------------+\n",
      "|    [[1], [2]]|                [1, 2]|                [1, 2]|\n",
      "|[[null], [55]]|            [null, 55]|            [null, 55]|\n",
      "+--------------+----------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"ArrayOfArrays\"),flatten(df.ArrayOfArrays),flatten(df.ArrayOfArrays)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE\n",
    "* If we apply multiple explote it throws error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+--------------------+\n",
      "|Empname|    Languages|          Properties|\n",
      "+-------+-------------+--------------------+\n",
      "| Alicia|[Java, Scala]|{Eye -> Brown, Ha...|\n",
      "| Robert|[Spark, Java]|{Eye -> null, Hai...|\n",
      "|   Mike|   [CSharp, ]|{Eye -> , Hair ->...|\n",
      "|   John|         null|                null|\n",
      "|   Jeff|       [1, 2]|                  {}|\n",
      "+-------+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Only one generator allowed per select clause but found 2: explode(Languages), explode(Properties)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5832\\250430474.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexplode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLanguages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mexplode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProperties\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Software_install\\Anaconda\\envs\\SPARK_HOME\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m   2021\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Alice'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Bob'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2022\u001b[0m         \"\"\"\n\u001b[1;32m-> 2023\u001b[1;33m         \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2024\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software_install\\Anaconda\\envs\\SPARK_HOME\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1322\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software_install\\Anaconda\\envs\\SPARK_HOME\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    194\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Only one generator allowed per select clause but found 2: explode(Languages), explode(Properties)"
     ]
    }
   ],
   "source": [
    "df.select(explode(df.Languages),explode(df.Properties)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formatting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+---------------------+--------+--------+------+\n",
      "|order_id|order_item_order_id|order_item_product_id|quantity|subtotal| price|\n",
      "+--------+-------------------+---------------------+--------+--------+------+\n",
      "|       1|                  1|                  957|       1|  299.98|299.98|\n",
      "|       2|                  2|                 1073|       1|  199.99|199.99|\n",
      "|       3|                  2|                  502|       5|   250.0|  50.0|\n",
      "+--------+-------------------+---------------------+--------+--------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.load('Order_items/part-00000',format=\"csv\",schema='order_id int,order_item_order_id int,order_item_product_id int, \\\n",
    "                   quantity int, subtotal float, price float')\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|  a|    b|\n",
      "+---+-----+\n",
      "|  5|Hello|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1=spark.createDataFrame([(5,\"Hello\")],(\"a\",\"b\"))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format_string\n",
    "* print the output like print statement in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------------------+\n",
      "|  a|    b|format_string(%d %s, a, b)|\n",
      "+---+-----+--------------------------+\n",
      "|  5|Hello|                   5 Hello|\n",
      "+---+-----+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select(col(\"*\"),format_string(\"%d %s\",df1.a,df1.b)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------------------------------------------------------------+\n",
      "|ID |Value                                                                    |\n",
      "+---+-------------------------------------------------------------------------+\n",
      "|1  | {\"Zipcode\":85016,\"Zipcodetype\":\"STANDARD\",\"City\":\"Phoenix\",\"State\":\"AZ\"}|\n",
      "+---+-------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(1,\"\"\" {\"Zipcode\":85016,\"Zipcodetype\":\"STANDARD\",\"City\":\"Phoenix\",\"State\":\"AZ\"}\"\"\")]\n",
    "df_map=spark.createDataFrame(data,(\"ID\",\"Value\"))\n",
    "df_map.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: long (nullable = true)\n",
      " |-- Value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_map.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "|ID |Value   |\n",
      "+---+--------+\n",
      "|1  | [1,2,3]|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(1,\"\"\" [1,2,3]\"\"\")]\n",
    "df_arr=spark.createDataFrame(data,(\"ID\",\"Value\"))\n",
    "df_arr.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: long (nullable = true)\n",
      " |-- Value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_arr.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------------------------------------------------------------+\n",
      "|ID |Value                                                                    |\n",
      "+---+-------------------------------------------------------------------------+\n",
      "|1  | {\"Zipcode\":85016,\"Zipcodetype\":\"STANDARD\",\"City\":\"Phoenix\",\"State\":\"AZ\"}|\n",
      "+---+-------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(1,\"\"\" {\"Zipcode\":85016,\"Zipcodetype\":\"STANDARD\",\"City\":\"Phoenix\",\"State\":\"AZ\"}\"\"\")]\n",
    "df_struct=spark.createDataFrame(data,(\"ID\",\"Value\"))\n",
    "df_struct.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From_json\n",
    "* Convert a json string into a MapType or Struct Type or ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| ID|               Value|\n",
      "+---+--------------------+\n",
      "|  1| {\"Zipcode\":85016...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_map.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------------------------------------------------------------+-------------------------------------------------------------------------+\n",
      "|ID |Value                                                                    |Map_column                                                               |\n",
      "+---+-------------------------------------------------------------------------+-------------------------------------------------------------------------+\n",
      "|1  | {\"Zipcode\":85016,\"Zipcodetype\":\"STANDARD\",\"City\":\"Phoenix\",\"State\":\"AZ\"}|{Zipcode -> 85016, Zipcodetype -> STANDARD, City -> Phoenix, State -> AZ}|\n",
      "+---+-------------------------------------------------------------------------+-------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema= MapType(StringType(),StringType())\n",
    "df_map.withColumn(\"Map_column\",from_json(df_map.Value,schema)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: long (nullable = true)\n",
      " |-- Value: string (nullable = true)\n",
      " |-- Map_column: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_nw=df_map.withColumn(\"Map_column\",from_json(df_map.Value,schema))\n",
    "df_nw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|Map_column[Zipcode]|\n",
      "+-------------------+\n",
      "|              85016|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_nw.select(df_nw.Map_column[\"Zipcode\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Json into Array TYpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| ID|   Value|\n",
      "+---+--------+\n",
      "|  1| [1,2,3]|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_arr.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+\n",
      "| ID|   Value|Arr_column|\n",
      "+---+--------+----------+\n",
      "|  1| [1,2,3]| [1, 2, 3]|\n",
      "+---+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema=ArrayType(IntegerType())\n",
    "df_array=df_arr.withColumn(\"Arr_column\",from_json(df_arr.Value,schema))\n",
    "df_array.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|Arr_column[0]|\n",
      "+-------------+\n",
      "|            1|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_array.select(df_array.Arr_column[0]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: long (nullable = true)\n",
      " |-- Value: string (nullable = true)\n",
      " |-- Arr_column: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_array.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From json to structType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------------------------------------------------------------+\n",
      "|ID |Value                                                                    |\n",
      "+---+-------------------------------------------------------------------------+\n",
      "|1  | {\"Zipcode\":85016,\"Zipcodetype\":\"STANDARD\",\"City\":\"Phoenix\",\"State\":\"AZ\"}|\n",
      "+---+-------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_struct.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema=StructType([StructField(\"Zipcode\",IntegerType()),StructField(\"Zipcodetype\",StringType()),\\\n",
    "                   StructField(\"City\",StringType()),StructField(\"State\",StringType())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------------------------------------------------------------+------------------------------+\n",
      "|ID |Value                                                                    |Struct_column                 |\n",
      "+---+-------------------------------------------------------------------------+------------------------------+\n",
      "|1  | {\"Zipcode\":85016,\"Zipcodetype\":\"STANDARD\",\"City\":\"Phoenix\",\"State\":\"AZ\"}|{85016, STANDARD, Phoenix, AZ}|\n",
      "+---+-------------------------------------------------------------------------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_struct.withColumn(\"Struct_column\",from_json(df_struct.Value,schema)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: long (nullable = true)\n",
      " |-- Value: string (nullable = true)\n",
      " |-- Struct_column: struct (nullable = true)\n",
      " |    |-- Zipcode: integer (nullable = true)\n",
      " |    |-- Zipcodetype: string (nullable = true)\n",
      " |    |-- City: string (nullable = true)\n",
      " |    |-- State: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_struct_nw=df_struct.withColumn(\"Struct_column\",from_json(df_struct.Value,schema))\n",
    "df_struct_nw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|Struct_column.Zipcode|\n",
      "+---------------------+\n",
      "|                85016|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_struct_nw.select(df_struct_nw.Struct_column.Zipcode).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO_json()\n",
    "* Converts map typr into String type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: long (nullable = true)\n",
      " |-- Value: string (nullable = true)\n",
      " |-- Map_column: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_nw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Map_column: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- to_json(Map_column): string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_nw.select(df_nw.Map_column,to_json(df_nw.Map_column)).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarly convert array type to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: long (nullable = true)\n",
      " |-- Value: string (nullable = true)\n",
      " |-- Arr_column: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_array.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Arr_column: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- to_json(Arr_column): string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_array.select(df_array.Arr_column,to_json(df_array.Arr_column)).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarly convert Struct type to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: long (nullable = true)\n",
      " |-- Value: string (nullable = true)\n",
      " |-- Struct_column: struct (nullable = true)\n",
      " |    |-- Zipcode: integer (nullable = true)\n",
      " |    |-- Zipcodetype: string (nullable = true)\n",
      " |    |-- City: string (nullable = true)\n",
      " |    |-- State: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_struct_nw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Struct_column: struct (nullable = true)\n",
      " |    |-- Zipcode: integer (nullable = true)\n",
      " |    |-- Zipcodetype: string (nullable = true)\n",
      " |    |-- City: string (nullable = true)\n",
      " |    |-- State: string (nullable = true)\n",
      " |-- to_json(Struct_column): string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_struct_nw.select(df_struct_nw.Struct_column,to_json(df_struct_nw.Struct_column)).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## json_tuple()\n",
    "* Extract elemnt from json string column and create the result as a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------------------------------------------------------------+\n",
      "|ID |Value                                                                    |\n",
      "+---+-------------------------------------------------------------------------+\n",
      "|1  | {\"Zipcode\":85016,\"Zipcodetype\":\"STANDARD\",\"City\":\"Phoenix\",\"State\":\"AZ\"}|\n",
      "+---+-------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_map.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Uisng on;y zipcode,city and state as the coumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+\n",
      "|  ZIP|  SEHER|RAJYE|\n",
      "+-----+-------+-----+\n",
      "|85016|Phoenix|   AZ|\n",
      "+-----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_map.select(json_tuple(df_map.Value,\"Zipcode\",\"City\",\"State\")).toDF(\"ZIP\",\"SEHER\",\"RAJYE\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_json_object()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------------------------------------------------------------+\n",
      "|ID |Value                                                                    |\n",
      "+---+-------------------------------------------------------------------------+\n",
      "|1  | {\"Zipcode\":85016,\"Zipcodetype\":\"STANDARD\",\"City\":\"Phoenix\",\"State\":\"AZ\"}|\n",
      "+---+-------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_map.select(col(\"*\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|  ZIP|\n",
      "+---+-----+\n",
      "|  1|85016|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_map.select(col(\"id\"),get_json_object(col(\"Value\"),\"$.Zipcode\").alias(\"ZIP\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:SPARK_HOME]",
   "language": "python",
   "name": "conda-env-SPARK_HOME-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
