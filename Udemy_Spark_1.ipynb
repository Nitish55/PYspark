{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* to get the partitons\n",
    "* converting pandas dataframe into Spark\n",
    "* Spark SQL\n",
    "* Global view\n",
    "* UDF--- Lambda\n",
    "* Structtype\n",
    "* Maptype\n",
    "* Arraytype\n",
    "* Row\n",
    "* Locate\n",
    "* SelectExpr\n",
    "* drop_duplicates\n",
    "* No of rows and columns\n",
    "* Sorting API\n",
    "* Set\n",
    "* Join(cross,leftanti,leftsemi,cross)\n",
    "* Aggregate(Summary,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql.functions import udf,col,length,expr,lit,lower,sumDistinct,sum,round,count,countDistinct,max,min\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"Training\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to get the partitons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "|   a| 10|\n",
      "|   b| 20|\n",
      "|   c| 30|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dd=pd.DataFrame(data={\"Name\":[\"a\",\"b\",\"c\"],\"Age\":[10,20,30]})\n",
    "df_emp=spark.createDataFrame(dd)\n",
    "df_emp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_emp.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## converting pandas dataframe into Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Name  Age\n",
       "0    a   10\n",
       "1    b   20\n",
       "2    c   30"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd=pd.DataFrame(data={\"Name\":[\"a\",\"b\",\"c\"],\"Age\":[10,20,30]})\n",
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "|   a| 10|\n",
      "|   b| 20|\n",
      "|   c| 30|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp=spark.createDataFrame(dd)\n",
    "df_emp.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_emp.createOrReplaceTempView(\"emp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "|   c| 30|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark.sql(\"\"\"select * from emp where Age >20\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+\n",
      "|Name|Age|Sex|\n",
      "+----+---+---+\n",
      "|  a1| 10|  M|\n",
      "|  b1| 20|  F|\n",
      "|  c1| 30|  M|\n",
      "+----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dg=pd.DataFrame(data={\"Name\":[\"a1\",\"b1\",\"c1\"],\"Age\":[10,20,30],\"Sex\":[\"M\",\"F\",\"M\"]})\n",
    "dep=spark.createDataFrame(dg)\n",
    "dep.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep.createOrReplaceGlobalTempView(\"jon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+\n",
      "|Name|Age|Sex|\n",
      "+----+---+---+\n",
      "|  c1| 30|  M|\n",
      "+----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select * from global_temp.jon where Age>20 and Sex='M'  \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "|   a| 10|\n",
      "|   b| 20|\n",
      "|   c| 30|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.table(\"emp\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reading text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|          value|\n",
      "+---------------+\n",
      "|  Name Age city|\n",
      "|Nitish 32 delhi|\n",
      "|  rakesh 33 goa|\n",
      "|   king 35 agra|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.text(\"tst.txt\",)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=\"nitish bhardwaj king is back\"\n",
    "def cps(st):\n",
    "    s=st.split(\" \")\n",
    "    cp=\"\"\n",
    "    for i in s:\n",
    "        cp=cp+i[0].upper()+i[1:]+\" \" \n",
    "    return cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nitish Bhardwaj King Is Back '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cps(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+\n",
      "|       Name|Age|\n",
      "+-----------+---+\n",
      "|chegiz khan| 10|\n",
      "| bryan fury| 20|\n",
      "| jin khazma| 30|\n",
      "+-----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dd=pd.DataFrame(data={\"Name\":[\"chegiz khan\",\"bryan fury\",\"jin khazma\"],\"Age\":[10,20,30]})\n",
    "df=spark.createDataFrame(dd)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UDF in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf,col,length\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf()\n",
    "def cps(st):\n",
    "    s=st.split(\" \")\n",
    "    cp=\"\"\n",
    "    for i in s:\n",
    "        cp=cp+i[0].upper()+i[1:]+\" \" \n",
    "    return cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+------------+\n",
      "|       Name|Age|   cps(Name)|\n",
      "+-----------+---+------------+\n",
      "|chegiz khan| 10|Chegiz Khan |\n",
      "| bryan fury| 20| Bryan Fury |\n",
      "| jin khazma| 30| Jin Khazma |\n",
      "+-----------+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"*\"),cps(df.Name)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lambda in spark/Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chegiz khan</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bryan fury</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jin khazma</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Name  Age  size\n",
       "0  chegiz khan   10    11\n",
       "1   bryan fury   20    10\n",
       "2   jin khazma   30    10"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd[\"size\"]=dd.apply(lambda x:len(x[\"Name\"]),axis=1)\n",
    "dd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+\n",
      "|       Name|Age|\n",
      "+-----------+---+\n",
      "|chegiz khan| 10|\n",
      "| bryan fury| 20|\n",
      "| jin khazma| 30|\n",
      "+-----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+------------+\n",
      "|       Name|Age|length(Name)|\n",
      "+-----------+---+------------+\n",
      "|chegiz khan| 10|          11|\n",
      "| bryan fury| 20|          10|\n",
      "| jin khazma| 30|          10|\n",
      "+-----------+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"*\"),length(df.Name)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(x)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spp=udf(lambda x:length(x),IntegerType())\n",
    "spp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|length(Name)|\n",
      "+------------+\n",
      "|          11|\n",
      "|          10|\n",
      "|          10|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(length(df.Name)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to check current database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'default'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentDatabase(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='default database', locationUri='file:/C:/Users/nitis/Downloads/Documents/PySpark/spark-warehouse')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField,IntegerType,StringType,DataType,MapType,ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema=StructType([\n",
    "    StructField(\"Name\",StringType(),True),\n",
    "    StructField(\"ID\",IntegerType(),True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name| ID|\n",
      "+-----+---+\n",
      "|James|  1|\n",
      "|Robet|  2|\n",
      "| John|  3|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(\"James\",1),\n",
    "     (\"Robet\",2),\n",
    "     (\"John\",3)]\n",
    "df=spark.createDataFrame(data=data,schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- ID: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema=StructType([\n",
    "    StructField(\"Name\",StringType(),True),\n",
    "    StructField(\"Properties\",MapType(StringType(),StringType()),True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------------+\n",
      "|Name |Properties                   |\n",
      "+-----+-----------------------------+\n",
      "|James|{eye -> Brown, hair -> black}|\n",
      "|Robet|{eye -> Black, hair -> White}|\n",
      "|John |{eye -> Blue, hair -> Blonde}|\n",
      "+-----+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(\"James\",{\"hair\":\"black\",\"eye\":\"Brown\"}),\n",
    "     (\"Robet\",{\"hair\":\"White\",\"eye\":\"Black\"}),\n",
    "     (\"John\",{\"hair\":\"Blonde\",\"eye\":\"Blue\"})]\n",
    "df=spark.createDataFrame(data=data,schema=schema)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|Properties[eye]|\n",
      "+---------------+\n",
      "|Brown          |\n",
      "|Black          |\n",
      "|Blue           |\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"Properties\")[\"eye\"]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema=StructType([\n",
    "    StructField(\"Name\",StringType(),True),\n",
    "    StructField(\"PhoneNum\",ArrayType(IntegerType()),True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|Name |PhoneNum  |\n",
      "+-----+----------+\n",
      "|James|[123, 900]|\n",
      "|Robet|[555, 999]|\n",
      "|John |[888, 100]|\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(\"James\",[123,900]),\n",
    "     (\"Robet\",[555,999]),\n",
    "     (\"John\",[888,100])]\n",
    "df=spark.createDataFrame(data=data,schema=schema)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- PhoneNum: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alex'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row=Row(Name=\"Alex\",Age=35)\n",
    "row.Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Name\" in row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Alex\" in row.Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"King\" in row.Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating rdd from row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Name='Alex', Age=35)\n",
      "Row(Name='Jin', Age=23)\n",
      "Row(Name='King', Age=33)\n",
      "Row(Name='John', Age=34)\n"
     ]
    }
   ],
   "source": [
    "lst=[Row(Name=\"Alex\",Age=35),Row(Name=\"Jin\",Age=23),Row(Name=\"King\",Age=33),Row(Name=\"John\",Age=34)]\n",
    "rdd=spark.sparkContext.parallelize(lst)\n",
    "for i in rdd.take(len(lst)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alex 35\n",
      "Jin 23\n",
      "King 33\n",
      "John 34\n"
     ]
    }
   ],
   "source": [
    "for i in rdd.take(len(lst)):\n",
    "    print(i.Name+\" \"+str(i.Age))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create dataframe using rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      "\n",
      "None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "|Alex| 35|\n",
      "| Jin| 23|\n",
      "|King| 33|\n",
      "|John| 34|\n",
      "+----+---+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "dt=spark.createDataFrame(lst)\n",
    "print(dt.printSchema())\n",
    "print(\"-\"*100)\n",
    "print(dt.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt=dt.withColumn(\"Age\",col(\"Age\").astype(\"int\"))\n",
    "dt.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Rows like Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'John'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person=Row(\"Name\",\"Age\")\n",
    "p1=person(\"John\",33)\n",
    "p2=person(\"Aman\",25)\n",
    "p1.Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count in rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=Row(Name=\"Alex\",Age=35,username=\"Alex\")\n",
    "p.count(\"Alex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### index in row\n",
    "* return first occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Name='Alex', Age=35, username='Alex')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.index(35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### asdict in rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Name': 'Alex', 'Age': 35, 'username': 'Alex'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.asDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternate of where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "|Alex| 35|\n",
      "| Jin| 23|\n",
      "|King| 33|\n",
      "|John| 34|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "|Alex| 35|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt[dt.Age==35].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "|Alex| 35|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt.where(dt.Age==35).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|length(Name)|\n",
      "+------------+\n",
      "|           4|\n",
      "|           3|\n",
      "|           4|\n",
      "|           4|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt.select(length(dt.Name)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----+\n",
      "|Name|Age|Vale|\n",
      "+----+---+----+\n",
      "|Alex| 35| lex|\n",
      "| Jin| 23|  in|\n",
      "|King| 33| ing|\n",
      "|John| 34| ohn|\n",
      "+----+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dt.withColumn(col(\"*\"),dt.Name.substr(2,int(length(dt.Name))).alias(\"test\")).show()\n",
    "\n",
    "dt.withColumn(\"Vale\", dt.Name.substr(lit(2), length(\"Name\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alex', 'Jin', 'King', 'John']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.Name for i in dt.select(dt[\"Name\"]).collect()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|            11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:00|             7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:00|             4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:00|             2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:00|             5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:00|             5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:00|              918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:00|             1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:00|             9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:00|             9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:00|             2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:00|             7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:00|             2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:00|             1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:00|             9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:00|             9198|     PROCESSING|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.load('orders/part-00000',format=\"csv\",schema='order_id int,order_date timestamp,order_customer_id int, \\\n",
    "                   order_status string')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lower-- Convert to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|lower(order_status)|\n",
      "+-------------------+\n",
      "|             closed|\n",
      "|    pending_payment|\n",
      "|           complete|\n",
      "|             closed|\n",
      "|           complete|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(lower(\"order_status\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## select expr--variant of select that accept SQL statemets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|SubString|\n",
      "+---------+\n",
      "|     2013|\n",
      "|     2013|\n",
      "|     2013|\n",
      "|     2013|\n",
      "|     2013|\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.order_date.substr(1,4).alias(\"SubString\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|new_order|\n",
      "+---------+\n",
      "|     2013|\n",
      "|     2013|\n",
      "|     2013|\n",
      "|     2013|\n",
      "|     2013|\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"substring(order_date,1,4) as new_order\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stack-- Used to unpivot the data, but not in Pyspark thats why we use stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'stack' from 'pyspark.sql.functions' (D:\\Software_install\\Anaconda\\envs\\SPARK_HOME\\lib\\site-packages\\pyspark\\sql\\functions.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9720\\1378080137.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m  \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstack\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'stack' from 'pyspark.sql.functions' (D:\\Software_install\\Anaconda\\envs\\SPARK_HOME\\lib\\site-packages\\pyspark\\sql\\functions.py)"
     ]
    }
   ],
   "source": [
    "from  pyspark.sql.functions import stack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nd=spark.range(1)\n",
    "nd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col0|col1|\n",
      "+----+----+\n",
      "|   1|   2|\n",
      "|   3|   4|\n",
      "|   5|   6|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nd.selectExpr(\"stack(3, 1,2,3,4,5,6)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* here value 1,2,3,4,5,6 is split into 3 rows as we have stack(3,-----)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|locate|\n",
      "+------+\n",
      "|     0|\n",
      "|     8|\n",
      "|     0|\n",
      "+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(F.locate(\"_\",df.order_status).alias(\"locate\")).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## drop_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|            12111|       COMPLETE|\n",
      "|      46|2013-07-25 00:00:00|             1549|        ON_HOLD|\n",
      "|      11|2013-07-25 00:00:00|              918| PAYMENT_REVIEW|\n",
      "|       8|2013-07-25 00:00:00|             2911|     PROCESSING|\n",
      "|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n",
      "|      69|2013-07-25 00:00:00|             2821|SUSPECTED_FRAUD|\n",
      "|      21|2013-07-25 00:00:00|             2711|        PENDING|\n",
      "|      50|2013-07-25 00:00:00|             5225|       CANCELED|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.drop_duplicates(subset=[\"order_status\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## diff between select and where/filter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+\n",
      "|((order_id > 10) AND (order_id < 20))|\n",
      "+-------------------------------------+\n",
      "|                                false|\n",
      "|                                false|\n",
      "|                                false|\n",
      "|                                false|\n",
      "|                                false|\n",
      "|                                false|\n",
      "|                                false|\n",
      "|                                false|\n",
      "|                                false|\n",
      "|                                false|\n",
      "|                                 true|\n",
      "+-------------------------------------+\n",
      "only showing top 11 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select((col(\"order_id\")>10) & (col(\"order_id\")<20)).show(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|      11|2013-07-25 00:00:00|              918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:00|             1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:00|             9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:00|             9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:00|             2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:00|             7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:00|             2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:00|             1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:00|             9488|PENDING_PAYMENT|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where((col(\"order_id\")>10) & (col(\"order_id\")<20)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ISIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+------------+\n",
      "|order_id|         order_date|order_customer_id|order_status|\n",
      "+--------+-------------------+-----------------+------------+\n",
      "|       1|2013-07-25 00:00:00|            11599|      CLOSED|\n",
      "|       3|2013-07-25 00:00:00|            12111|    COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|             8827|      CLOSED|\n",
      "|       5|2013-07-25 00:00:00|            11318|    COMPLETE|\n",
      "|       6|2013-07-25 00:00:00|             7130|    COMPLETE|\n",
      "|       7|2013-07-25 00:00:00|             4530|    COMPLETE|\n",
      "|      12|2013-07-25 00:00:00|             1837|      CLOSED|\n",
      "|      15|2013-07-25 00:00:00|             2568|    COMPLETE|\n",
      "|      17|2013-07-25 00:00:00|             2667|    COMPLETE|\n",
      "|      18|2013-07-25 00:00:00|             1205|      CLOSED|\n",
      "|      22|2013-07-25 00:00:00|              333|    COMPLETE|\n",
      "|      24|2013-07-25 00:00:00|            11441|      CLOSED|\n",
      "|      25|2013-07-25 00:00:00|             9503|      CLOSED|\n",
      "|      26|2013-07-25 00:00:00|             7562|    COMPLETE|\n",
      "|      28|2013-07-25 00:00:00|              656|    COMPLETE|\n",
      "|      32|2013-07-25 00:00:00|             3960|    COMPLETE|\n",
      "|      35|2013-07-25 00:00:00|             4840|    COMPLETE|\n",
      "|      37|2013-07-25 00:00:00|             5863|      CLOSED|\n",
      "|      45|2013-07-25 00:00:00|             2636|    COMPLETE|\n",
      "|      51|2013-07-25 00:00:00|            12271|      CLOSED|\n",
      "+--------+-------------------+-----------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(col(\"order_status\").isin(\"CLOSED\",\"COMPLETE\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No of rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Rows count : 68883\n",
      "No of columns are :4\n"
     ]
    }
   ],
   "source": [
    "rows=df.count()\n",
    "print(f\"DataFrame Rows count : {rows}\")\n",
    "cols=len(df.columns)\n",
    "print(f\"No of columns are :{cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting API\n",
    "* we can use order_by() or sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|   41643|2014-04-08 00:00:00|            12435|        PENDING|\n",
      "|   61629|2013-12-21 00:00:00|            12435|       CANCELED|\n",
      "|    6160|2013-09-02 00:00:00|            12434|       COMPLETE|\n",
      "|    4799|2013-08-23 00:00:00|            12434|PENDING_PAYMENT|\n",
      "|   42915|2014-04-16 00:00:00|            12434|       COMPLETE|\n",
      "|    1868|2013-08-03 00:00:00|            12434|         CLOSED|\n",
      "|   13544|2013-10-16 00:00:00|            12434|        PENDING|\n",
      "|   51800|2014-06-14 00:00:00|            12434|        ON_HOLD|\n",
      "|   61777|2013-12-26 00:00:00|            12434|       COMPLETE|\n",
      "|    5303|2013-08-26 00:00:00|            12434|        PENDING|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(col(\"order_customer_id\").desc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|   22945|2013-12-13 00:00:00|                1|       COMPLETE|\n",
      "|   15192|2013-10-29 00:00:00|                2|PENDING_PAYMENT|\n",
      "|   33865|2014-02-18 00:00:00|                2|       COMPLETE|\n",
      "|   57963|2013-08-02 00:00:00|                2|        ON_HOLD|\n",
      "|   67863|2013-11-30 00:00:00|                2|       COMPLETE|\n",
      "|   35158|2014-02-26 00:00:00|                3|       COMPLETE|\n",
      "|   22646|2013-12-11 00:00:00|                3|       COMPLETE|\n",
      "|   57617|2014-07-24 00:00:00|                3|       COMPLETE|\n",
      "|   23662|2013-12-19 00:00:00|                3|       COMPLETE|\n",
      "|   56178|2014-07-15 00:00:00|                3|        PENDING|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(col(\"order_customer_id\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sorting multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|   22945|2013-12-13 00:00:00|                1|       COMPLETE|\n",
      "|   67863|2013-11-30 00:00:00|                2|       COMPLETE|\n",
      "|   33865|2014-02-18 00:00:00|                2|       COMPLETE|\n",
      "|   57963|2013-08-02 00:00:00|                2|        ON_HOLD|\n",
      "|   15192|2013-10-29 00:00:00|                2|PENDING_PAYMENT|\n",
      "|   23662|2013-12-19 00:00:00|                3|       COMPLETE|\n",
      "|   22646|2013-12-11 00:00:00|                3|       COMPLETE|\n",
      "|   57617|2014-07-24 00:00:00|                3|       COMPLETE|\n",
      "|   35158|2014-02-26 00:00:00|                3|       COMPLETE|\n",
      "|   61453|2013-12-14 00:00:00|                3|       COMPLETE|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(col(\"order_customer_id\"),col(\"order_status\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doing asceding and descing on columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|   61629|2013-12-21 00:00:00|            12435|       CANCELED|\n",
      "|   41643|2014-04-08 00:00:00|            12435|        PENDING|\n",
      "|    1868|2013-08-03 00:00:00|            12434|         CLOSED|\n",
      "|    6160|2013-09-02 00:00:00|            12434|       COMPLETE|\n",
      "|   61777|2013-12-26 00:00:00|            12434|       COMPLETE|\n",
      "|   42915|2014-04-16 00:00:00|            12434|       COMPLETE|\n",
      "|   51800|2014-06-14 00:00:00|            12434|        ON_HOLD|\n",
      "|    5303|2013-08-26 00:00:00|            12434|        PENDING|\n",
      "|   13544|2013-10-16 00:00:00|            12434|        PENDING|\n",
      "|    4799|2013-08-23 00:00:00|            12434|PENDING_PAYMENT|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(col(\"order_customer_id\"),col(\"order_status\"),ascending=[0,1]).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort Within Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "|   a|   1|\n",
      "|   b|   2|\n",
      "|   c|   3|\n",
      "|   d|   4|\n",
      "|   e|   5|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt=[(\"a\",1),(\"b\",2),(\"c\",3),(\"d\",4),(\"e\",5)]\n",
    "dd=spark.createDataFrame(dt,schema=(\"col1\",\"col2\"))\n",
    "dd.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get the num of partionss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check how data is distributed in these partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Row(col1='a', col2=1)],\n",
       " [Row(col1='b', col2=2)],\n",
       " [Row(col1='c', col2=3)],\n",
       " [Row(col1='d', col2=4), Row(col1='e', col2=5)]]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## here\n",
    "* in partion 1 we have [Row(col1='a', col2=1)]\n",
    "* similarly in Partiotn 4 we have [Row(col1='d', col2=4), Row(col1='e', col2=5)]]\n",
    "\n",
    "## now if we use sort() it sort globally i.e alpplied on whole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## if we use sort Within partion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "|   a|   1|\n",
      "|   b|   2|\n",
      "|   c|   3|\n",
      "|   e|   5|\n",
      "|   d|   4|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dd.sortWithinPartitions(dd.col1.desc()).show()\n",
    "## check only partion is applied soed so we are getting \"e\" followed by \"d\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1=spark.range(10)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2=spark.range(5,15)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.union(df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "|   a|   1|\n",
      "|   b|   2|\n",
      "+----+----+\n",
      "\n",
      "None\n",
      "--------------------------------------------------\n",
      "+----+----+\n",
      "|col2|col1|\n",
      "+----+----+\n",
      "|   b|   3|\n",
      "|   d|   4|\n",
      "+----+----+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df1=spark.createDataFrame(data=[(\"a\",1),(\"b\",2)],schema=(\"col1 string,col2 int\"))\n",
    "df2=spark.createDataFrame(data=[(\"b\",3),(\"d\",4)],schema=(\"col2 string,col1 int\"))\n",
    "print(df1.show())\n",
    "print(\"-\"*50)\n",
    "print(df2.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "|   a|   1|\n",
      "|   b|   2|\n",
      "|   b|   3|\n",
      "|   d|   4|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## if we do union\n",
    "df1.union(df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "|   a|   1|\n",
      "|   b|   2|\n",
      "|   3|   b|\n",
      "|   4|   d|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## if we do unionByName we get\n",
    "df1.unionByName(df2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intersect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "|   a|   1|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1=spark.createDataFrame(data=[(\"a\",1),(\"a\",1),(\"b\",2)],schema=(\"col1 string,col2 int\"))\n",
    "df2=spark.createDataFrame(data=[(\"a\",1),(\"a\",1),(\"c\",4)],schema=(\"col2 string,col1 int\"))\n",
    "df1.intersect(df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "|   a|   1|\n",
      "|   a|   1|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.intersectAll(df2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Except"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "|   b|   2|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.exceptAll(df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col2|col1|\n",
      "+----+----+\n",
      "|   c|   4|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.exceptAll(df1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|empID|Country|\n",
      "+-----+-------+\n",
      "|    2|    USA|\n",
      "|    4|  India|\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1=spark.createDataFrame([(1,\"Robert\"),(2,\"julie\"),(3,\"James\")],schema=[\"empID\",\"empName\"])\n",
    "df2=spark.createDataFrame([(2,\"USA\"),(4,\"India\")],schema=[\"empID\",\"Country\"])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inner_Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------+\n",
      "|empID|empName|Country|\n",
      "+-----+-------+-------+\n",
      "|    2|  julie|    USA|\n",
      "+-----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2,on=\"empID\",how=\"inner\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+-------+\n",
      "|empID|empName|empID|Country|\n",
      "+-----+-------+-----+-------+\n",
      "|    2|  julie|    2|    USA|\n",
      "+-----+-------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2,df1.empID==df2.empID).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| ID|Country|\n",
      "+---+-------+\n",
      "|  2|    USA|\n",
      "|  4|  India|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1=spark.createDataFrame([(1,\"Robert\"),(2,\"julie\"),(3,\"James\")],schema=[\"empID\",\"empName\"])\n",
    "df2=spark.createDataFrame([(2,\"USA\"),(4,\"India\")],schema=[\"ID\",\"Country\"])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+---+-------+\n",
      "|empID|empName| ID|Country|\n",
      "+-----+-------+---+-------+\n",
      "|    2|  julie|  2|    USA|\n",
      "+-----+-------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2,df1.empID==df2.ID,how=\"inner\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| ID|empID|\n",
      "+---+-----+\n",
      "|  2|    2|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2,df1.empID==df2.ID,how=\"inner\").select(\"ID\",\"empID\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Left join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+-------+\n",
      "|empID|empName|empID|Country|\n",
      "+-----+-------+-----+-------+\n",
      "|    1| Robert| null|   null|\n",
      "|    2|  julie|    2|    USA|\n",
      "|    3|  James| null|   null|\n",
      "+-----+-------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1=spark.createDataFrame([(1,\"Robert\"),(2,\"julie\"),(3,\"James\")],schema=[\"empID\",\"empName\"])\n",
    "df2=spark.createDataFrame([(2,\"USA\"),(4,\"India\")],schema=[\"empID\",\"Country\"])\n",
    "df1.join(df2,df1.empID==df2.empID,how=\"left\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|empID|Country|\n",
      "+-----+-------+\n",
      "|    1|   null|\n",
      "|    2|    USA|\n",
      "|    3|   null|\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2,df1.empID==df2.empID,how=\"left\").select(df1.empID,\"Country\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|empID|Country|\n",
      "+-----+-------+\n",
      "|    2|    USA|\n",
      "| null|  India|\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2,df1.empID==df2.empID,how=\"right\").select(df1.empID,\"Country\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full===(Left Join + Right Join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+-------+\n",
      "|empID|empName|empID|Country|\n",
      "+-----+-------+-----+-------+\n",
      "|    1| Robert| null|   null|\n",
      "|    2|  julie|    2|    USA|\n",
      "|    3|  James| null|   null|\n",
      "| null|   null|    4|  India|\n",
      "+-----+-------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2,df1.empID==df2.empID,how=\"full\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Join---> For Cartesian product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+-------+\n",
      "|empID|empName|empID|Country|\n",
      "+-----+-------+-----+-------+\n",
      "|    2|  julie|    2|    USA|\n",
      "+-----+-------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2,df1.empID==df2.empID,how=\"cross\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Left Anti join--- Only those recored will be the output ehich were not present in second dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|empID|empName|\n",
      "+-----+-------+\n",
      "|    1| Robert|\n",
      "|    3|  James|\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2,df1.empID==df2.empID,how=\"leftanti\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeftSemi-- Similar to inner join, only diff is that we cant select column from the second table in select statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|empID|empName|\n",
      "+-----+-------+\n",
      "|    2|  julie|\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2,df1.empID==df2.empID,how=\"leftsemi\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Resolved attribute(s) Country#170 missing from empID#165L,empName#166 in operator !Project [empID#165L, Country#170].;\n!Project [empID#165L, Country#170]\n+- Join LeftSemi, (empID#165L = empID#169L)\n   :- LogicalRDD [empID#165L, empName#166], false\n   +- LogicalRDD [empID#169L, Country#170], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3096\\445059964.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempID\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempID\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"leftsemi\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempID\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCountry\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Software_install\\Anaconda\\envs\\SPARK_HOME\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m   2021\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Alice'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Bob'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2022\u001b[0m         \"\"\"\n\u001b[1;32m-> 2023\u001b[1;33m         \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2024\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software_install\\Anaconda\\envs\\SPARK_HOME\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1322\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software_install\\Anaconda\\envs\\SPARK_HOME\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    194\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Resolved attribute(s) Country#170 missing from empID#165L,empName#166 in operator !Project [empID#165L, Country#170].;\n!Project [empID#165L, Country#170]\n+- Join LeftSemi, (empID#165L = empID#169L)\n   :- LogicalRDD [empID#165L, empName#166], false\n   +- LogicalRDD [empID#169L, Country#170], false\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2,df1.empID==df2.empID,how=\"leftsemi\").select(df1.empID,df2.Country).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+---------+\n",
      "|empID|empName|managerid|\n",
      "+-----+-------+---------+\n",
      "|    1| Robert|        2|\n",
      "|    2|  Jenny|        3|\n",
      "|    3|  James|        5|\n",
      "+-----+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.createDataFrame([(1,\"Robert\",2),(2,\"Jenny\",3),(3,\"James\",5)],schema=\"empID int,empName string,managerid int\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+---------+------------+\n",
      "|empID|empName|managerid|Manager_name|\n",
      "+-----+-------+---------+------------+\n",
      "|    1| Robert|        2|       Jenny|\n",
      "|    2|  Jenny|        3|       James|\n",
      "|    3|  James|        5|        null|\n",
      "+-----+-------+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.alias(\"d1\").join(df.alias(\"d2\"),col(\"d1.managerid\")==col(\"d2.empID\"),\"left\").select(col(\"d1.*\"),\\\n",
    "                                                                                       col(\"d2.empName\").alias(\"Manager_name\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining using multiple variables(composite variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-------+\n",
      "|empID|deptid|empName|\n",
      "+-----+------+-------+\n",
      "|    1|   101| Robert|\n",
      "|    2|   102|  julie|\n",
      "|    3|   103|  James|\n",
      "+-----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1=spark.createDataFrame([(1,101,\"Robert\"),(2,102,\"julie\"),(3,103,\"James\")],schema=\"empID int,deptid int,empName string\")\n",
    "df2=spark.createDataFrame([(2,102,\"USA\"),(4,104,\"India\")],schema=\"empID int,deptid int,empName string\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-------+-----+------+-------+\n",
      "|empID|deptid|empName|empID|deptid|empName|\n",
      "+-----+------+-------+-----+------+-------+\n",
      "|    2|   102|  julie|    2|   102|    USA|\n",
      "+-----+------+-------+-----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2,(df1.empID==df2.empID) & (df1.deptid==df2.deptid),how=\"inner\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiframe dataframe join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|empID|country|\n",
      "+-----+-------+\n",
      "|    2|    USA|\n",
      "|    4|  India|\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1=spark.createDataFrame([(1,\"Robert\"),(2,\"Jenny\"),(3,\"James\")],schema=\"empID int,empName string\")\n",
    "df2=spark.createDataFrame([(2,\"USA\"),(4,\"India\")],schema=\"empID int,country string\")\n",
    "df3=spark.createDataFrame([(1,\"01-jan-2021\"),(2,\"01-feb-2021\"),(3,\"01-mar-2021\")],schema=\"empID int,joindate string\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+-------+-----+-----------+\n",
      "|empID|empName|empID|country|empID|   joindate|\n",
      "+-----+-------+-----+-------+-----+-----------+\n",
      "|    2|  Jenny|    2|    USA|    2|01-feb-2021|\n",
      "+-----+-------+-----+-------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2,df1.empID==df2.empID,how=\"inner\").join(df3,df1.empID==df3.empID,how=\"inner\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+---------------------+--------+--------+------+\n",
      "|order_id|order_item_order_id|order_item_product_id|quantity|subtotal| price|\n",
      "+--------+-------------------+---------------------+--------+--------+------+\n",
      "|       1|                  1|                  957|       1|  299.98|299.98|\n",
      "|       2|                  2|                 1073|       1|  199.99|199.99|\n",
      "|       3|                  2|                  502|       5|   250.0|  50.0|\n",
      "+--------+-------------------+---------------------+--------+--------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.load('Order_items/part-00000',format=\"csv\",schema='order_id int,order_item_order_id int,order_item_product_id int, \\\n",
    "                   quantity int, subtotal float, price float')\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_item_order_id: integer (nullable = true)\n",
      " |-- order_item_product_id: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- subtotal: float (nullable = true)\n",
      " |-- price: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-------------------+---------------------+------------------+------------------+------------------+\n",
      "|summary|         order_id|order_item_order_id|order_item_product_id|          quantity|          subtotal|             price|\n",
      "+-------+-----------------+-------------------+---------------------+------------------+------------------+------------------+\n",
      "|  count|           172198|             172198|               172198|            172198|            172198|            172198|\n",
      "|   mean|          86099.5|  34442.56682423721|    660.4877176273824|2.1821275508426345|199.32066922046081|133.75906959048717|\n",
      "| stddev|49709.42516431533| 19883.325171992343|     310.514472790008|1.4663523175387134|112.74303987146804|118.55893633258484|\n",
      "|    min|                1|                  1|                   19|                 1|              9.99|              9.99|\n",
      "|    25%|            43033|              17204|                  403|                 1|            119.98|              50.0|\n",
      "|    50%|            86085|              34464|                  627|                 1|            199.92|             59.99|\n",
      "|    75%|           129134|              51682|                 1004|                 3|            299.95|            199.99|\n",
      "|    max|           172198|              68883|                 1073|                 5|           1999.99|           1999.99|\n",
      "+-------+-----------------+-------------------+---------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|summary|             price|          subtotal|\n",
      "+-------+------------------+------------------+\n",
      "|  count|            172198|            172198|\n",
      "|   mean|133.75906959048717|199.32066922046081|\n",
      "| stddev|118.55893633258484|112.74303987146804|\n",
      "|    min|              9.99|              9.99|\n",
      "|    25%|              50.0|            119.98|\n",
      "|    50%|             59.99|            199.92|\n",
      "|    75%|            199.99|            299.95|\n",
      "|    max|           1999.99|           1999.99|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.summary().select(\"summary\",\"price\",\"subtotal\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum and sumDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software_install\\Anaconda\\envs\\SPARK_HOME\\lib\\site-packages\\pyspark\\sql\\functions.py:315: FutureWarning: Deprecated in 3.2, use sum_distinct instead.\n",
      "  warnings.warn(\"Deprecated in 3.2, use sum_distinct instead.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+\n",
      "|    Sum_price|Sum_Distinct|\n",
      "+-------------+------------+\n",
      "|2.303304427E7|     9832.42|\n",
      "+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(round(sum(\"price\"),2).alias(\"Sum_price\"),round(sumDistinct(\"price\"),2).alias(\"Sum_Distinct\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count and CountDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n",
      "|Count_price|Count_Distinct|\n",
      "+-----------+--------------+\n",
      "|     172198|            57|\n",
      "+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(count(\"price\").alias(\"Count_price\"),countDistinct(\"price\").alias(\"Count_Distinct\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|max(price)|\n",
      "+----------+\n",
      "|   1999.99|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(max(\"price\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|min(price)|\n",
      "+----------+\n",
      "|      9.99|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(min(\"price\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## collect_list and collect_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|salary|\n",
      "+---+------+\n",
      "|  1|   100|\n",
      "|  2|   150|\n",
      "|  3|   200|\n",
      "|  4|    50|\n",
      "|  5|   500|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.createDataFrame([(1,100),(2,150),(3,200),(4,50),(5,500)],schema=\"id int,salary int\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_list,collect_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+------------------------+\n",
      "|list                    |set                     |\n",
      "+------------------------+------------------------+\n",
      "|[100, 150, 200, 50, 500]|[150, 100, 50, 500, 200]|\n",
      "+------------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(collect_list(\"salary\").alias(\"list\"),collect_set(\"salary\").alias(\"set\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### here\n",
    "* list contain all salary and set contain distinct salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:SPARK_HOME]",
   "language": "python",
   "name": "conda-env-SPARK_HOME-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
