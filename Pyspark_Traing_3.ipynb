{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Collect\n",
    "* Sample\n",
    "* fill and fillna()\n",
    "* Pivot() Unpivot()\n",
    "* Create PySpark ArrayType\n",
    "* map()\n",
    "* explode()\n",
    "* split()\n",
    "* Maptype()\n",
    "* when()\n",
    "* lit()\n",
    "* split()\n",
    "* concat_ws\n",
    "* substring()\n",
    "*  regexp_replace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.types import StructField, StructType, StringType, MapType\n",
    "from pyspark.sql.functions import col,lit,udf,expr,map_keys,explode,map_values,when,concat,split,concat_ws,substring,translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName(\"Train_Hard\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect\n",
    "\n",
    "* PySpark RDD/DataFrame collect() is an action operation that is used to retrieve all the elements of the dataset (from all nodes) to the driver node. We should use the collect() on smaller dataset usually after filter(), group() e.t.c. Retrieving larger datasets results in OutOfMemory error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|  Finance|     10|\n",
      "|Marketing|     20|\n",
      "|    Sales|     30|\n",
      "|       IT|     40|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept = [(\"Finance\",10), \\\n",
    "    (\"Marketing\",20), \\\n",
    "    (\"Sales\",30), \\\n",
    "    (\"IT\",40) \\\n",
    "  ]\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
    "deptDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(dept_name='Finance', dept_id=10), Row(dept_name='Marketing', dept_id=20), Row(dept_name='Sales', dept_id=30), Row(dept_name='IT', dept_id=40)]\n"
     ]
    }
   ],
   "source": [
    "dta_collect=deptDF.collect()\n",
    "print(dta_collect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note that collect() is an action hence it does not return a DataFrame instead, it returns data in an Array to the driver. Once the data is in an array, you can use python for loop to process it further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finance\n",
      "Marketing\n",
      "Sales\n",
      "IT\n"
     ]
    }
   ],
   "source": [
    "for i in dta_collect:\n",
    "    print(i[\"dept_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample\n",
    "* PySpark sampling (pyspark.sql.DataFrame.sample()) is a mechanism to get random sample records from the dataset, this is helpful when you have a larger dataset and wanted to analyze/test a subset of the data for example 10% of the original file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.range(100)\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.1 returns 10% of the rows. However, this does not guarantee it returns the exact 10% of the records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(.1).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(id=2), Row(id=6), Row(id=27), Row(id=38), Row(id=47), Row(id=53), Row(id=61), Row(id=67), Row(id=75), Row(id=77), Row(id=82), Row(id=94)]\n"
     ]
    }
   ],
   "source": [
    "print(df.sample(.1).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using seed to reproduce the same Samples in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(id=4), Row(id=13), Row(id=16), Row(id=22), Row(id=32), Row(id=39), Row(id=93), Row(id=94)]\n"
     ]
    }
   ],
   "source": [
    "print(df.sample(.1,seed=12).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(id=4), Row(id=13), Row(id=16), Row(id=22), Row(id=32), Row(id=39), Row(id=93), Row(id=94)]\n"
     ]
    }
   ],
   "source": [
    "print(df.sample(.1,seed=12).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample withReplacement (May contain duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(id=5), Row(id=15), Row(id=16), Row(id=43), Row(id=62), Row(id=64), Row(id=64), Row(id=65), Row(id=74), Row(id=76), Row(id=80)]\n"
     ]
    }
   ],
   "source": [
    "print(df.sample(True,.1,seed=12).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill() and Fillna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "|     null|  34|        10| 38000|\n",
      "|     null|  36|      null|  null|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.csv(\"test1.csv\",header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- Experience: string (nullable = true)\n",
      " |-- Salary: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Experience: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=df.withColumn(\"age\",(col(\"age\").cast(\"int\")))\\\n",
    "  .withColumn(\"Salary\",(col(\"Salary\").cast(\"int\")))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace NULL/None Values with Zero (0)\n",
    "* it will change the values of only integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "|   Mahesh|  0|      null| 40000|\n",
      "|     null| 34|        10| 38000|\n",
      "|     null| 36|      null|     0|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(value=0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace null values with \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|          | 40000|\n",
      "|         |  34|        10| 38000|\n",
      "|         |  36|          |  null|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(\" \").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* note it is aaplied only on strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filling only a particular colum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "|  missing|  34|        10| 38000|\n",
      "|  missing|  36|      null|  null|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(\"missing\",subset=[\"Name\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fillna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "|   Mahesh|  0|      null| 40000|\n",
      "|     null| 34|        10| 38000|\n",
      "|     null| 36|      null|     0|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.fillna(0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|   missing| 40000|\n",
      "|     null|  34|        10| 38000|\n",
      "|     null|  36|   missing|  null|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.fillna(\"missing\",subset=[\"Experience\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot\n",
    "* PySpark pivot() function is used to rotate/transpose the data from one column into multiple Dataframe columns and back using unpivot().\n",
    "* it is an aggregation where one of the grouping columns values is transposed into individual columns with distinct data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Amount: long (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "+-------+------+-------+\n",
      "|Product|Amount|Country|\n",
      "+-------+------+-------+\n",
      "|Banana |1000  |USA    |\n",
      "|Carrots|1500  |USA    |\n",
      "|Beans  |1600  |USA    |\n",
      "|Orange |2000  |USA    |\n",
      "|Orange |2000  |USA    |\n",
      "|Banana |400   |China  |\n",
      "|Carrots|1200  |China  |\n",
      "|Beans  |1500  |China  |\n",
      "|Orange |4000  |China  |\n",
      "|Banana |2000  |Canada |\n",
      "|Carrots|2000  |Canada |\n",
      "|Beans  |2000  |Mexico |\n",
      "+-------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"Banana\",1000,\"USA\"), (\"Carrots\",1500,\"USA\"), (\"Beans\",1600,\"USA\"), \\\n",
    "      (\"Orange\",2000,\"USA\"),(\"Orange\",2000,\"USA\"),(\"Banana\",400,\"China\"), \\\n",
    "      (\"Carrots\",1200,\"China\"),(\"Beans\",1500,\"China\"),(\"Orange\",4000,\"China\"), \\\n",
    "      (\"Banana\",2000,\"Canada\"),(\"Carrots\",2000,\"Canada\"),(\"Beans\",2000,\"Mexico\")]\n",
    "\n",
    "columns= [\"Product\",\"Amount\",\"Country\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----------+\n",
      "|Product|country|sum(Amount)|\n",
      "+-------+-------+-----------+\n",
      "|Carrots|    USA|       1500|\n",
      "| Banana|    USA|       1000|\n",
      "|  Beans|    USA|       1600|\n",
      "| Banana|  China|        400|\n",
      "| Orange|    USA|       4000|\n",
      "| Orange|  China|       4000|\n",
      "|Carrots|  China|       1200|\n",
      "|  Beans|  China|       1500|\n",
      "|Carrots| Canada|       2000|\n",
      "|  Beans| Mexico|       2000|\n",
      "| Banana| Canada|       2000|\n",
      "+-------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy([\"Product\",\"country\"]).sum(\"Amount\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----+------+----+\n",
      "|Product|Canada|China|Mexico| USA|\n",
      "+-------+------+-----+------+----+\n",
      "| Orange|  null| 4000|  null|4000|\n",
      "|  Beans|  null| 1500|  2000|1600|\n",
      "| Banana|  2000|  400|  null|1000|\n",
      "|Carrots|  2000| 1200|  null|1500|\n",
      "+-------+------+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pi=df.groupBy(\"Product\").pivot(\"country\").sum(\"Amount\")\n",
    "pi.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+------+------+\n",
      "|Product| USA|China|Canada|Mexico|\n",
      "+-------+----+-----+------+------+\n",
      "| Orange|4000| 4000|  null|  null|\n",
      "|  Beans|1600| 1500|  null|  2000|\n",
      "| Banana|1000|  400|  2000|  null|\n",
      "|Carrots|1500| 1200|  2000|  null|\n",
      "+-------+----+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## rearranging columns\n",
    "countries = [\"USA\",\"China\",\"Canada\",\"Mexico\"]\n",
    "pivotDF =df.groupBy(\"Product\").pivot(\"country\",countries).sum(\"Amount\")\n",
    "pivotDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpivot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Product|Country|Total|\n",
      "+-------+-------+-----+\n",
      "|Orange |China  |4000 |\n",
      "|Beans  |China  |1500 |\n",
      "|Beans  |Mexico |2000 |\n",
      "|Banana |Canada |2000 |\n",
      "|Banana |China  |400  |\n",
      "|Carrots|Canada |2000 |\n",
      "|Carrots|China  |1200 |\n",
      "+-------+-------+-----+\n",
      "\n",
      "+-------+-------+-----+\n",
      "|Product|Country|Total|\n",
      "+-------+-------+-----+\n",
      "| Orange|  China| 4000|\n",
      "|  Beans|  China| 1500|\n",
      "|  Beans| Mexico| 2000|\n",
      "| Banana| Canada| 2000|\n",
      "| Banana|  China|  400|\n",
      "|Carrots| Canada| 2000|\n",
      "|Carrots|  China| 1200|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unpivotExpr = \"stack(3, 'Canada', Canada, 'China', China, 'Mexico', Mexico) as (Country,Total)\"\n",
    "unPivotDF = pivotDF.select(\"Product\", expr(unpivotExpr)) \\\n",
    "    .where(\"Total is not null\")\n",
    "unPivotDF.show(truncate=False)\n",
    "unPivotDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PySpark ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- languagesAtSchool: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- languagesAtWork: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- currentState: string (nullable = true)\n",
      " |-- previousState: string (nullable = true)\n",
      "\n",
      "+----------------+------------------+---------------+------------+-------------+\n",
      "|            name| languagesAtSchool|languagesAtWork|currentState|previousState|\n",
      "+----------------+------------------+---------------+------------+-------------+\n",
      "|    James,,Smith|[Java, Scala, C++]|  [Spark, Java]|          OH|           CA|\n",
      "|   Michael,Rose,|[Spark, Java, C++]|  [Spark, Java]|          NY|           NJ|\n",
      "|Robert,,Williams|      [CSharp, VB]|[Spark, Python]|          UT|           NV|\n",
      "+----------------+------------------+---------------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = [\n",
    " (\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"],\"OH\",\"CA\"),\n",
    " (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"],\"NY\",\"NJ\"),\n",
    " (\"Robert,,Williams\",[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"],\"UT\",\"NV\")\n",
    "]\n",
    "\n",
    "from pyspark.sql.types import StringType, ArrayType,StructType,StructField\n",
    "schema = StructType([ \n",
    "    StructField(\"name\",StringType(),True), \n",
    "    StructField(\"languagesAtSchool\",ArrayType(StringType()),True), \n",
    "    StructField(\"languagesAtWork\",ArrayType(StringType()),True), \n",
    "    StructField(\"currentState\", StringType(), True), \n",
    "    StructField(\"previousState\", StringType(), True)\n",
    "  ])\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## changing columns name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "can't set attribute",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5380\\3035457415.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"c1\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"c2\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"c3\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"c4\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"c5\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: can't set attribute"
     ]
    }
   ],
   "source": [
    "df.columns=[\"c1\",\"c2\",\"c3\",\"c4\",\"c5\"]\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+---------------+---+---+\n",
      "|              c1|                c2|             c3| c4| c5|\n",
      "+----------------+------------------+---------------+---+---+\n",
      "|    James,,Smith|[Java, Scala, C++]|  [Spark, Java]| OH| CA|\n",
      "|   Michael,Rose,|[Spark, Java, C++]|  [Spark, Java]| NY| NJ|\n",
      "|Robert,,Williams|      [CSharp, VB]|[Spark, Python]| UT| NV|\n",
      "+----------------+------------------+---------------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#rdd=spark.sparkContext.parallelize(data)\n",
    "cols=[\"c1\",\"c2\",\"c3\",\"c4\",\"c5\"]\n",
    "ndd=df.rdd.map(lambda x: (x[0],x[1],x[2],x[3],x[4]))\n",
    "df2=ndd.toDF(cols)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+\n",
      "|            name|   col|\n",
      "+----------------+------+\n",
      "|    James,,Smith|  Java|\n",
      "|    James,,Smith| Scala|\n",
      "|    James,,Smith|   C++|\n",
      "|   Michael,Rose,| Spark|\n",
      "|   Michael,Rose,|  Java|\n",
      "|   Michael,Rose,|   C++|\n",
      "|Robert,,Williams|CSharp|\n",
      "|Robert,,Williams|    VB|\n",
      "+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "df.select(df.name,explode(df.languagesAtSchool)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         nameAsArray|\n",
      "+--------------------+\n",
      "|    [James, , Smith]|\n",
      "|   [Michael, Rose, ]|\n",
      "|[Robert, , Williams]|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "df.select(split(df.name,\",\").alias(\"nameAsArray\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# array()\n",
    "* Use array() function to create a new array column by merging the data from multiple columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+\n",
      "|            name| New_col|\n",
      "+----------------+--------+\n",
      "|    James,,Smith|[OH, CA]|\n",
      "|   Michael,Rose,|[NY, NJ]|\n",
      "|Robert,,Williams|[UT, NV]|\n",
      "+----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array\n",
    "df.select(df.name,array(\"currentState\",\"previousState\").alias(\"New_col\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## array_contains() \n",
    "* sql function is used to check if array column contains a value. Returns null if the array is null, true if the array contains the value, and false otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+\n",
      "|            name|array_contains|\n",
      "+----------------+--------------+\n",
      "|    James,,Smith|          true|\n",
      "|   Michael,Rose,|          true|\n",
      "|Robert,,Williams|         false|\n",
      "+----------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import array_contains\n",
    "df.select(df.name,array_contains(df.languagesAtSchool,\"Java\")\n",
    "    .alias(\"array_contains\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapType()\n",
    "* pySpark MapType (also called map type) is a data type to represent Python Dictionary (dict) to store key-value pair, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The First param keyType is used to specify the type of the key in the map.\n",
    "* The Second param valueType is used to specify the type of the value in the map.\n",
    "* Third parm valueContainsNull is an optional boolean type that is used to specify if the value of the second param can accept Null/None values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------------+\n",
      "|name      |properties                   |\n",
      "+----------+-----------------------------+\n",
      "|James     |{eye -> brown, hair -> black}|\n",
      "|Michael   |{eye -> null, hair -> brown} |\n",
      "|Robert    |{eye -> black, hair -> red}  |\n",
      "|Washington|{eye -> grey, hair -> grey}  |\n",
      "|Jefferson |{eye -> , hair -> brown}     |\n",
      "+----------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('properties', MapType(StringType(),StringType()),True)\n",
    "])\n",
    "dd = [('James',{'hair':'black','eye':'brown'}),\n",
    "        ('Michael',{'hair':'brown','eye':None}),\n",
    "        ('Robert',{'hair':'red','eye':'black'}),\n",
    "        ('Washington',{'hair':'grey','eye':'grey'}),\n",
    "        ('Jefferson',{'hair':'brown','eye':''})]\n",
    "df=spark.createDataFrame(dd,schema)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access PySpark MapType Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----+-----+\n",
      "|      name|          properties|  eye| hair|\n",
      "+----------+--------------------+-----+-----+\n",
      "|     James|{eye -> brown, ha...|brown|black|\n",
      "|   Michael|{eye -> null, hai...| null|brown|\n",
      "|    Robert|{eye -> black, ha...|black|  red|\n",
      "|Washington|{eye -> grey, hai...| grey| grey|\n",
      "| Jefferson|{eye -> , hair ->...|     |brown|\n",
      "+----------+--------------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"eye\",df.properties.getItem(\"eye\"))\\\n",
    ".withColumn(\"hair\",df.properties.getItem(\"hair\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+\n",
      "|      name|  eye| hair|\n",
      "+----------+-----+-----+\n",
      "|     James|brown|black|\n",
      "|   Michael| null|brown|\n",
      "|    Robert|black|  red|\n",
      "|Washington| grey| grey|\n",
      "| Jefferson|     |brown|\n",
      "+----------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"eye\",df.properties.getItem(\"eye\"))\\\n",
    ".withColumn(\"hair\",df.properties.getItem(\"hair\"))\\\n",
    ".drop(\"properties\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using explode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+\n",
      "|      name| key|value|\n",
      "+----------+----+-----+\n",
      "|     James| eye|brown|\n",
      "|     James|hair|black|\n",
      "|   Michael| eye| null|\n",
      "|   Michael|hair|brown|\n",
      "|    Robert| eye|black|\n",
      "|    Robert|hair|  red|\n",
      "|Washington| eye| grey|\n",
      "|Washington|hair| grey|\n",
      "| Jefferson| eye|     |\n",
      "| Jefferson|hair|brown|\n",
      "+----------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.name,explode(df.properties)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map_keys() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|      name|map_keys(properties)|\n",
      "+----------+--------------------+\n",
      "|     James|         [eye, hair]|\n",
      "|   Michael|         [eye, hair]|\n",
      "|    Robert|         [eye, hair]|\n",
      "|Washington|         [eye, hair]|\n",
      "| Jefferson|         [eye, hair]|\n",
      "+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.name,map_keys(df.properties)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|      name|       New_Col|\n",
      "+----------+--------------+\n",
      "|     James|[brown, black]|\n",
      "|   Michael| [null, brown]|\n",
      "|    Robert|  [black, red]|\n",
      "|Washington|  [grey, grey]|\n",
      "| Jefferson|     [, brown]|\n",
      "+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.name,map_values(df.properties).alias(\"New_Col\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+\n",
      "|   name|gender|salary|\n",
      "+-------+------+------+\n",
      "|  James|     M| 60000|\n",
      "|Michael|     M| 70000|\n",
      "| Robert|  null|400000|\n",
      "|  Maria|     F|500000|\n",
      "|    Jen|      |  null|\n",
      "+-------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"James\",\"M\",60000),(\"Michael\",\"M\",70000),\n",
    "        (\"Robert\",None,400000),(\"Maria\",\"F\",500000),\n",
    "        (\"Jen\",\"\",None)]\n",
    "\n",
    "columns = [\"name\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+-------+\n",
      "|   name|gender|salary|new_age|\n",
      "+-------+------+------+-------+\n",
      "|  James|     M| 60000|   Male|\n",
      "|Michael|     M| 70000|   Male|\n",
      "| Robert|  null|400000|   null|\n",
      "|  Maria|     F|500000| Female|\n",
      "|    Jen|      |  null|       |\n",
      "+-------+------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"new_age\",when(df.gender==\"M\",\"Male\")\n",
    "             .when(df.gender==\"F\",\"Female\")\n",
    "             .otherwise(df.gender)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+-------+\n",
      "|   name|gender|salary|new_age|\n",
      "+-------+------+------+-------+\n",
      "|  James|     M| 60000|   Male|\n",
      "|Michael|     M| 70000|   Male|\n",
      "| Robert|  null|400000|missing|\n",
      "|  Maria|     F|500000| Female|\n",
      "|    Jen|      |  null|       |\n",
      "+-------+------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"new_age\",when(df.gender==\"M\",\"Male\")\n",
    "             .when(df.gender==\"F\",\"Female\")\n",
    "             .when(df.gender.isNull(),\"missing\")\n",
    "             .otherwise(df.gender)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using case when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+-------+\n",
      "|   name|gender|salary|new_gen|\n",
      "+-------+------+------+-------+\n",
      "|  James|     M| 60000|   Male|\n",
      "|Michael|     M| 70000|   Male|\n",
      "| Robert|  null|400000|       |\n",
      "|  Maria|     F|500000| Female|\n",
      "|    Jen|      |  null|       |\n",
      "+-------+------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"new_gen\",expr(\"CASE WHEN gender = 'M' THEN 'Male' \" + \n",
    "               \"WHEN gender = 'F' THEN 'Female' WHEN gender IS NULL THEN ''\" +\n",
    "               \"ELSE gender END\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Conditions using & and | operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+----------+\n",
      "|   name|gender|salary|new_column|\n",
      "+-------+------+------+----------+\n",
      "|  James|     M| 60000|     ladka|\n",
      "|Michael|     M| 70000|    Baccha|\n",
      "| Robert|  null|400000|    Baccha|\n",
      "|  Maria|     F|500000|      ldki|\n",
      "|    Jen|      |  null|    Baccha|\n",
      "+-------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"new_column\",when((df.gender==\"M\") & (df.name==\"James\"),\"ladka\")\n",
    "             .when((df.name==\"Maria\")| (df.gender==\"F\"),\"ldki\")\n",
    "             .otherwise(\"Baccha\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark expr()\n",
    "* PySpark expr() is a SQL function to execute SQL-like expressions and to use an existing DataFrame column value as an expression argument to Pyspark built-in functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate Columns using expr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----------+\n",
      "| col1| col2|       Name|\n",
      "+-----+-----+-----------+\n",
      "|James| Bond| James,Bond|\n",
      "|Scott|Varsa|Scott,Varsa|\n",
      "+-----+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(\"James\",\"Bond\"),(\"Scott\",\"Varsa\")] \n",
    "df=spark.createDataFrame(data).toDF(\"col1\",\"col2\") \n",
    "df.withColumn(\"Name\",expr(\"col1 ||','||col2\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----------+\n",
      "| col1| col2|      Name|\n",
      "+-----+-----+----------+\n",
      "|James| Bond| JamesBond|\n",
      "|Scott|Varsa|ScottVarsa|\n",
      "+-----+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"Name\",concat(col(\"col1\"),col(\"col2\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an Existing Column Value for Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|      date|increment|\n",
      "+----------+---------+\n",
      "|2019-01-23|        1|\n",
      "|2019-06-24|        2|\n",
      "|2019-09-20|        3|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(\"2019-01-23\",1),(\"2019-06-24\",2),(\"2019-09-20\",3)] \n",
    "df=spark.createDataFrame(data).toDF(\"date\",\"increment\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+\n",
      "|      date|increment|  inc_data|\n",
      "+----------+---------+----------+\n",
      "|2019-01-23|        1|2019-02-23|\n",
      "|2019-06-24|        2|2019-08-24|\n",
      "|2019-09-20|        3|2019-12-20|\n",
      "+----------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"*\"),expr(\"add_months(date,increment)\").alias(\"inc_data\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arithmetic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+\n",
      "|      date|increment|new_increment|\n",
      "+----------+---------+-------------+\n",
      "|2019-01-23|        1|            6|\n",
      "|2019-06-24|        2|            7|\n",
      "|2019-09-20|        3|            8|\n",
      "+----------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"*\"),expr(\"increment +5 as new_increment\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Using Filter with expr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "| 100|   2|\n",
      "| 200|3000|\n",
      "| 500| 500|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(100,2),(200,3000),(500,500)] \n",
    "df=spark.createDataFrame(data).toDF(\"col1\",\"col2\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "| 500| 500|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(expr(\"col1==col2\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lit\n",
    "* PySpark SQL functions lit() and typedLit() are used to add a new column to DataFrame by assigning a literal or constant value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|EmpId|Salary|\n",
      "+-----+------+\n",
      "|  111| 50000|\n",
      "|  222| 60000|\n",
      "|  333| 40000|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"111\",50000),(\"222\",60000),(\"333\",40000)]\n",
    "columns= [\"EmpId\",\"Salary\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### usage of lit() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+--------+\n",
      "|EmpId|Salary|constant|\n",
      "+-----+------+--------+\n",
      "|  111| 50000|       1|\n",
      "|  222| 60000|       1|\n",
      "|  333| 40000|       1|\n",
      "+-----+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"*\"),lit(1).alias(\"constant\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q- if salary is ge 40000 and le 50000 then 1000 else 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|EmpId|Salary|\n",
      "+-----+------+\n",
      "|  111| 50000|\n",
      "|  333| 40000|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df.withColumn(\"new_val\",when(col(\"Salary\")>=4000 & col(\"Salary\")<=5000,lit(\"100\")).otherwise(lit(\"200\")))\n",
    "df.filter((col(\"Salary\") >=40000) &( col(\"Salary\") <= 50000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-------+\n",
      "|EmpId|Salary|new_val|\n",
      "+-----+------+-------+\n",
      "|  111| 50000|    100|\n",
      "|  222| 60000|    200|\n",
      "|  333| 40000|    100|\n",
      "+-----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"new_val\",when((col(\"Salary\") >=40000) &( col(\"Salary\") <= 50000),lit(\"100\")).otherwise(lit(\"200\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split()\n",
    "* PySpark SQL provides split() function to convert delimiter separated String to an Array (StringType to ArrayType) column on DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+------+------+\n",
      "|                name|dob_year|gender|salary|\n",
      "+--------------------+--------+------+------+\n",
      "|     James, A, Smith|    2018|     M|  3000|\n",
      "|Michael, Rose, Jones|    2010|     M|  4000|\n",
      "|   Robert,K,Williams|    2010|     M|  4000|\n",
      "|    Maria,Anne,Jones|    2005|     F|  4000|\n",
      "|      Jen,Mary,Brown|    2010|      |    -1|\n",
      "+--------------------+--------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"James, A, Smith\",\"2018\",\"M\",3000),\n",
    "            (\"Michael, Rose, Jones\",\"2010\",\"M\",4000),\n",
    "            (\"Robert,K,Williams\",\"2010\",\"M\",4000),\n",
    "            (\"Maria,Anne,Jones\",\"2005\",\"F\",4000),\n",
    "            (\"Jen,Mary,Brown\",\"2010\",\"\",-1)\n",
    "            ]\n",
    "\n",
    "columns=[\"name\",\"dob_year\",\"gender\",\"salary\"]\n",
    "df=spark.createDataFrame(data,columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|Name_array              |\n",
      "+------------------------+\n",
      "|[James,  A,  Smith]     |\n",
      "|[Michael,  Rose,  Jones]|\n",
      "|[Robert, K, Williams]   |\n",
      "|[Maria, Anne, Jones]    |\n",
      "|[Jen, Mary, Brown]      |\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(split(col(\"name\"),\",\").alias(\"Name_array\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|Name_array|\n",
      "+----------+\n",
      "|James     |\n",
      "|Michael   |\n",
      "|Robert    |\n",
      "|Maria     |\n",
      "|Jen       |\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(split(col(\"name\"),\",\")[0].alias(\"Name_array\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# concat_ws()\n",
    "* takes delimiter of your choice as a first argument and array column (type Column) as the second argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+------------+\n",
      "|name            |languagesAtSchool |currentState|\n",
      "+----------------+------------------+------------+\n",
      "|James,,Smith    |[Java, Scala, C++]|CA          |\n",
      "|Michael,Rose,   |[Spark, Java, C++]|NJ          |\n",
      "|Robert,,Williams|[CSharp, VB]      |NV          |\n",
      "+----------------+------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "columns = [\"name\",\"languagesAtSchool\",\"currentState\"]\n",
    "data = [(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \\\n",
    "    (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"), \\\n",
    "    (\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In order to convert array to a string, PySpark SQL provides a built-in function concat_ws() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------+------------+\n",
      "|            name|languagesAtSchool|currentState|\n",
      "+----------------+-----------------+------------+\n",
      "|    James,,Smith|   Java,Scala,C++|          CA|\n",
      "|   Michael,Rose,|   Spark,Java,C++|          NJ|\n",
      "|Robert,,Williams|        CSharp,VB|          NV|\n",
      "+----------------+-----------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"languagesAtSchool\",concat_ws(\",\",col(\"languagesAtSchool\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Substring()\n",
    "* substring() function is used to extract the substring from a DataFrame string column by providing the position and length of the string you wanted to extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|    date|\n",
      "+---+--------+\n",
      "|  1|20200828|\n",
      "|  2|20180525|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = [(1,\"20200828\"),(2,\"20180525\")]\n",
    "columns=[\"id\",\"date\"]\n",
    "df=spark.createDataFrame(data,columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----+-----+---+\n",
      "| id|    date|year|month|day|\n",
      "+---+--------+----+-----+---+\n",
      "|  1|20200828|2020|   08| 28|\n",
      "|  2|20180525|2018|   05| 25|\n",
      "+---+--------+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"year\",substring(\"date\",1,4)) \\\n",
    "              .withColumn(\"month\",substring(\"date\",5,2))\\\n",
    "              .withColumn(\"day\",substring(col(\"date\"),7,2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translate()\n",
    "* By using translate() string function you can replace character by character of DataFrame column value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----+\n",
      "| id|           address|state|\n",
      "+---+------------------+-----+\n",
      "|  1|  14851 Jeffrey Rd|   DE|\n",
      "|  2|43421 Margarita St|   NY|\n",
      "|  3|  13111 Siemon Ave|   CA|\n",
      "+---+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "address = [(1,\"14851 Jeffrey Rd\",\"DE\"),\n",
    "    (2,\"43421 Margarita St\",\"NY\"),\n",
    "    (3,\"13111 Siemon Ave\",\"CA\")]\n",
    "df =spark.createDataFrame(address,[\"id\",\"address\",\"state\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----+------------------+\n",
      "| id|           address|state|               add|\n",
      "+---+------------------+-----+------------------+\n",
      "|  1|  14851 Jeffrey Rd|   DE|  A485A Jeffrey Rd|\n",
      "|  2|43421 Margarita St|   NY|4C4BA Margarita St|\n",
      "|  3|  13111 Siemon Ave|   CA|  ACAAA Siemon Ave|\n",
      "+---+------------------+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"add\",translate(col(\"address\"),\"123\",\"ABC\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# regexp_replace()\n",
    "* you can replace a column value with a string for another string/substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----+\n",
      "| id|           address|state|\n",
      "+---+------------------+-----+\n",
      "|  1|14851 Jeffrey Road|   DE|\n",
      "|  2|43421 Margarita St|   NY|\n",
      "|  3|  13111 Siemon Ave|   CA|\n",
      "+---+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "df.withColumn('address', regexp_replace(\"address\",\"Rd\",\"Road\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----+----------------------+\n",
      "|id |address           |state|new_add               |\n",
      "+---+------------------+-----+----------------------+\n",
      "|1  |14851 Jeffrey Rd  |DE   |14851 Jeffrey Road    |\n",
      "|2  |43421 Margarita St|NY   |43421 Margarita Street|\n",
      "|3  |13111 Siemon Ave  |CA   |13111 Siemon Avenue   |\n",
      "+---+------------------+-----+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('new_add', \n",
    "              when(df.address.endswith(\"Rd\"),regexp_replace(\"address\",\"Rd\",\"Road\")) \\\n",
    "             .when(df.address.endswith(\"St\"),regexp_replace(\"address\",\"St\",\"Street\"))\\\n",
    "             .when(df.address.endswith(\"Ave\"),regexp_replace(\"address\",\"Ave\",\"Avenue\"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# translate()\n",
    "* string function you can replace character by character of DataFrame column value.\n",
    "* In the below example, every character of 1 is replaced with A, 2 replaced with B, and 3 replaced with C on the address column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----+------------------+\n",
      "| id|           address|state|              adds|\n",
      "+---+------------------+-----+------------------+\n",
      "|  1|  14851 Jeffrey Rd|   DE|  A485A Jeffrey Rd|\n",
      "|  2|43421 Margarita St|   NY|4C4BA Margarita St|\n",
      "|  3|  13111 Siemon Ave|   CA|  ACAAA Siemon Ave|\n",
      "+---+------------------+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import translate\n",
    "df.withColumn('adds', translate(\"address\",\"123\",\"ABC\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace Column with Another Column Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----+\n",
      "|     col1|col2|col3|\n",
      "+---------+----+----+\n",
      "|ABCDE_XYZ| XYZ| FGH|\n",
      "+---------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "   [(\"ABCDE_XYZ\", \"XYZ\",\"FGH\")], \n",
    "    (\"col1\", \"col2\",\"col3\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----+---------+\n",
      "|     col1|col2|col3|     col4|\n",
      "+---------+----+----+---------+\n",
      "|ABCDE_XYZ| XYZ| FGH|ABCDE_XYZ|\n",
      "+---------+----+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"col4\",regexp_replace(\"col1\",\"col2\",\"col3\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----+---------+\n",
      "|     col1|col2|col3|     col4|\n",
      "+---------+----+----+---------+\n",
      "|ABCDE_XYZ| XYZ| FGH|ABCDE_FGH|\n",
      "+---------+----+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"col4\",expr(\"regexp_replace(col1,col2,col3)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----+---------+\n",
      "|     col1|col2|col3|     col4|\n",
      "+---------+----+----+---------+\n",
      "|ABCDE_XYZ| XYZ| FGH|ABCDE_FGH|\n",
      "+---------+----+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"col4\",regexp_replace(\"col1\",\"XYZ\",\"FGH\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overlay()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+\n",
      "|     col1|col2|\n",
      "+---------+----+\n",
      "|ABCDE_XYZ| FGH|\n",
      "+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"ABCDE_XYZ\", \"FGH\")], (\"col1\", \"col2\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|overlayed|\n",
      "+---------+\n",
      "|ABCDE_FGH|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import overlay\n",
    "df.select(overlay(\"col1\", \"col2\", 7).alias(\"overlayed\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "| overlayed|\n",
      "+----------+\n",
      "|ABCDE_XFGH|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(overlay(\"col1\", \"col2\", 8).alias(\"overlayed\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|  overlayed|\n",
      "+-----------+\n",
      "|ABCDE_XYFGH|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(overlay(\"col1\", \"col2\", 9).alias(\"overlayed\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|   overlayed|\n",
      "+------------+\n",
      "|ABCDE_XYZFGH|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(overlay(\"col1\", \"col2\", 10).alias(\"overlayed\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|   overlayed|\n",
      "+------------+\n",
      "|ABCDE_XYZFGH|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(overlay(\"col1\", \"col2\", 12).alias(\"overlayed\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:SPARK_HOME]",
   "language": "python",
   "name": "conda-env-SPARK_HOME-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
