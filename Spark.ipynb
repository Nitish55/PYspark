{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyspark\n",
    "# Install the 'findsparkâ€™ Python module through the Anaconda Prompt or Terminal by running \n",
    "# python -m pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\nitis\\\\Downloads\\\\Documents\\\\PySpark'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|hello|\n",
      "+-----+\n",
      "|spark|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql('''select \"spark\" as hello''')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using SPARK\n",
    "spark=SparkSession.builder.appName(\"Practise\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-7L78A1J:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x255e7ea1208>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading VIA Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark=spark.read.csv(\"sample.csv\")\n",
    "df_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+-------------+\n",
      "|  _c0|_c1|   _c2|          _c3|\n",
      "+-----+---+------+-------------+\n",
      "| Name|Age|Salary|         City|\n",
      "|Ankit| 22|   100|        Delhi|\n",
      "|Mohit| 21|   200|Muzaffarnagar|\n",
      "| Aman| 11|   250|       Meerut|\n",
      "|Rahul| 33|   300|          Goa|\n",
      "+-----+---+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: string, Salary: string, City: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## In order to read the data excluding _c0,_C1,_c2 and _C3\n",
    "df_spark=spark.read.option(\"header\",\"true\").csv(\"Sample.csv\")\n",
    "df_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+-------------+\n",
      "| Name|Age|Salary|         City|\n",
      "+-----+---+------+-------------+\n",
      "|Ankit| 22|   100|        Delhi|\n",
      "|Mohit| 21|   200|Muzaffarnagar|\n",
      "| Aman| 11|   250|       Meerut|\n",
      "|Rahul| 33|   300|          Goa|\n",
      "+-----+---+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.option(\"header\",\"true\").csv(\"Sample.csv\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Checking Type\n",
    "type(df_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Name='Ankit', Age='22', Salary='100', City='Delhi')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Checking diff diff meathods\n",
    "df_spark.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-7L78A1J:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x20614019848>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark=SparkSession.builder.appName(\"DataFrame\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: string, Salary: string, City: string]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Reading Dataset\n",
    "df_spark=spark.read.option(\"header\",\"true\").csv(\"Sample.csv\")\n",
    "df_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+-------------+\n",
      "| Name|Age|Salary|         City|\n",
      "+-----+---+------+-------------+\n",
      "|Ankit| 22|   100|        Delhi|\n",
      "|Mohit| 21|   200|Muzaffarnagar|\n",
      "| Aman| 11|   250|       Meerut|\n",
      "|Rahul| 33|   300|          Goa|\n",
      "+-----+---+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Salary: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Checking the Schema(data_types)\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note Here--\n",
    "* all columns are taken as string even Age, Salary to solve this we have to use\n",
    "\n",
    "`inferschema=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark=spark.read.option(\"header\",\"true\").csv(\"Sample.csv\",inferSchema=True)\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>Salary</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ankit</td>\n",
       "      <td>22</td>\n",
       "      <td>100</td>\n",
       "      <td>Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mohit</td>\n",
       "      <td>21</td>\n",
       "      <td>200</td>\n",
       "      <td>Muzaffarnagar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aman</td>\n",
       "      <td>11</td>\n",
       "      <td>250</td>\n",
       "      <td>Meerut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rahul</td>\n",
       "      <td>33</td>\n",
       "      <td>300</td>\n",
       "      <td>Goa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Name  Age  Salary           City\n",
       "0  Ankit   22     100          Delhi\n",
       "1  Mohit   21     200  Muzaffarnagar\n",
       "2   Aman   11     250         Meerut\n",
       "3  Rahul   33     300            Goa"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"sample.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Difference out the string and integer variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Age', 'Salary']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## using the pandas\n",
    "[i for i in df.columns if df[i].dtypes==\"int64\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Age', 'Salary']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vl=[i for i in df_spark.columns if df_spark.select([i]).dtypes[0][1]==\"int\"]\n",
    "vl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meathod-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: int, Salary: int, City: string]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark=spark.read.csv(\"Sample.csv\",header=True,inferSchema=True)\n",
    "df_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+-------------+\n",
      "| Name|Age|Salary|         City|\n",
      "+-----+---+------+-------------+\n",
      "|Ankit| 22|   100|        Delhi|\n",
      "|Mohit| 21|   200|Muzaffarnagar|\n",
      "| Aman| 11|   250|       Meerut|\n",
      "|Rahul| 33|   300|          Goa|\n",
      "+-----+---+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'Age', 'Salary', 'City']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## To get all the columns\n",
    "df_spark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='Ankit', Age=22, Salary=100, City='Delhi'),\n",
       " Row(Name='Mohit', Age=21, Salary=200, City='Muzaffarnagar')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## seleting top 3 records\n",
    "df_spark.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diference between pandas head and pyspark head is that\n",
    "* In pyspark we get output in the form of list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>Salary</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ankit</td>\n",
       "      <td>22</td>\n",
       "      <td>100</td>\n",
       "      <td>Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mohit</td>\n",
       "      <td>21</td>\n",
       "      <td>200</td>\n",
       "      <td>Muzaffarnagar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Name  Age  Salary           City\n",
       "0  Ankit   22     100          Delhi\n",
       "1  Mohit   21     200  Muzaffarnagar"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+-------------+\n",
      "| Name|Age|Salary|         City|\n",
      "+-----+---+------+-------------+\n",
      "|Ankit| 22|   100|        Delhi|\n",
      "|Mohit| 21|   200|Muzaffarnagar|\n",
      "| Aman| 11|   250|       Meerut|\n",
      "|Rahul| 33|   300|          Goa|\n",
      "+-----+---+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Ankit| 22|\n",
      "|Mohit| 21|\n",
      "| Aman| 11|\n",
      "|Rahul| 33|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.select(\"Name\",\"Age\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'), ('Age', 'int'), ('Salary', 'int'), ('City', 'string')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+----------------+-----------------+-------------+\n",
      "|summary| Name|             Age|           Salary|         City|\n",
      "+-------+-----+----------------+-----------------+-------------+\n",
      "|  count|    4|               4|                4|            4|\n",
      "|   mean| null|           21.75|            212.5|         null|\n",
      "| stddev| null|8.99536917900909|85.39125638299666|         null|\n",
      "|    min| Aman|              11|              100|        Delhi|\n",
      "|    max|Rahul|              33|              300|Muzaffarnagar|\n",
      "+-------+-----+----------------+-----------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding columns in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+-------------+----------+\n",
      "| Name|Age|Salary|         City|Experience|\n",
      "+-----+---+------+-------------+----------+\n",
      "|Ankit| 22|   100|        Delhi|        12|\n",
      "|Mohit| 21|   200|Muzaffarnagar|        11|\n",
      "| Aman| 11|   250|       Meerut|         1|\n",
      "|Rahul| 33|   300|          Goa|        23|\n",
      "+-----+---+------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.withColumn(\"Experience\",df_spark[\"Age\"]-10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+-------------+\n",
      "| Name|Age|Salary|         City|\n",
      "+-----+---+------+-------------+\n",
      "|Ankit| 22|   100|        Delhi|\n",
      "|Mohit| 21|   200|Muzaffarnagar|\n",
      "| Aman| 11|   250|       Meerut|\n",
      "|Rahul| 33|   300|          Goa|\n",
      "+-----+---+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+-------------+----------+\n",
      "| Name|Age|Salary|         City|Experience|\n",
      "+-----+---+------+-------------+----------+\n",
      "|Ankit| 22|   100|        Delhi|        12|\n",
      "|Mohit| 21|   200|Muzaffarnagar|        11|\n",
      "| Aman| 11|   250|       Meerut|         1|\n",
      "|Rahul| 33|   300|          Goa|        23|\n",
      "+-----+---+------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark=df_spark.withColumn(\"Experience\",df_spark[\"Age\"]-10)\n",
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+-------------+----------+\n",
      "| Name|Age|Salary|         City|Experience|\n",
      "+-----+---+------+-------------+----------+\n",
      "|Ankit| 22|   100|        Delhi|        12|\n",
      "|Mohit| 21|   200|Muzaffarnagar|        11|\n",
      "| Aman| 11|   250|       Meerut|         1|\n",
      "|Rahul| 33|   300|          Goa|        23|\n",
      "+-----+---+------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.drop(\"Experience\")\n",
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Element has not removed coz thee i no inplace operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+-------------+\n",
      "| Name|Age|Salary|         City|\n",
      "+-----+---+------+-------------+\n",
      "|Ankit| 22|   100|        Delhi|\n",
      "|Mohit| 21|   200|Muzaffarnagar|\n",
      "| Aman| 11|   250|       Meerut|\n",
      "|Rahul| 33|   300|          Goa|\n",
      "+-----+---+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark=df_spark.drop(\"Experience\")\n",
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Renaming the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+------+-------------+\n",
      "|New_Name|Age|Salary|         City|\n",
      "+--------+---+------+-------------+\n",
      "|   Ankit| 22|   100|        Delhi|\n",
      "|   Mohit| 21|   200|Muzaffarnagar|\n",
      "|    Aman| 11|   250|       Meerut|\n",
      "|   Rahul| 33|   300|          Goa|\n",
      "+--------+---+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.withColumnRenamed(\"Name\",\"New_Name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter -3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, age: int, Experience: int, Salary: int]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## reading the new dataset\n",
    "spark.read.csv(\"test1.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "|     null|  34|        10| 38000|\n",
      "|     null|  36|      null|  null|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.csv(\"test1.csv\",header=True,inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dropping null values from all the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How=\"Any\" and \"all\" in drop\n",
    "* how=\"all\" --> means if any value in the row is null it get drop\n",
    "* how=\"all\" --> means if all the values in the row is null it get dropped\n",
    "* By default how=\"any\" in the drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "|     null|  34|        10| 38000|\n",
      "|     null|  36|      null|  null|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(how=\"all\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(how=\"any\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold\n",
    "* if we put thresh=2, it means if the row dont have 2 non-null values it got deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "|     null|  34|        10| 38000|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(how=\"any\",thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "|     null|  34|        10| 38000|\n",
      "|     null|  36|      null|  null|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset\n",
    "* when want to delete the vaue from a specific column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(how=\"any\",subset=[\"Name\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(how=\"any\",subset=[\"Name\",\"age\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+----------+------+\n",
      "|          Name| age|Experience|Salary|\n",
      "+--------------+----+----------+------+\n",
      "|         Krish|  31|        10| 30000|\n",
      "|     Sudhanshu|  30|         8| 25000|\n",
      "|         Sunny|  29|         4| 20000|\n",
      "|          Paul|  24|         3| 20000|\n",
      "|        Harsha|  21|         1| 15000|\n",
      "|       Shubham|  23|         2| 18000|\n",
      "|        Mahesh|null|      null| 40000|\n",
      "|missing_values|  34|        10| 38000|\n",
      "|missing_values|  36|      null|  null|\n",
      "+--------------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(\"missing_values\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In order to fill only a aspecific column use subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+----------+------+\n",
      "|          Name| age|Experience|Salary|\n",
      "+--------------+----+----------+------+\n",
      "|         Krish|  31|        10| 30000|\n",
      "|     Sudhanshu|  30|         8| 25000|\n",
      "|         Sunny|  29|         4| 20000|\n",
      "|          Paul|  24|         3| 20000|\n",
      "|        Harsha|  21|         1| 15000|\n",
      "|       Shubham|  23|         2| 18000|\n",
      "|        Mahesh|null|      null| 40000|\n",
      "|missing_values|  34|        10| 38000|\n",
      "|missing_values|  36|      null|  null|\n",
      "+--------------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(\"missing_values\",subset=[\"Name\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing numeric column with mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "impute=Imputer(\n",
    "    inputCols=[\"age\",\"Experience\",\"Salary\"],\n",
    "              outputCols=[\"{}_Imputed\".format(c) for c  in [\"age\",\"Experience\",\"Salary\"]]## creating new columns\n",
    "                                                                                           #having imputed value\n",
    ").setStrategy(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+-----------+------------------+--------------+\n",
      "|     Name| age|Experience|Salary|age_Imputed|Experience_Imputed|Salary_Imputed|\n",
      "+---------+----+----------+------+-----------+------------------+--------------+\n",
      "|    Krish|  31|        10| 30000|         31|                10|         30000|\n",
      "|Sudhanshu|  30|         8| 25000|         30|                 8|         25000|\n",
      "|    Sunny|  29|         4| 20000|         29|                 4|         20000|\n",
      "|     Paul|  24|         3| 20000|         24|                 3|         20000|\n",
      "|   Harsha|  21|         1| 15000|         21|                 1|         15000|\n",
      "|  Shubham|  23|         2| 18000|         23|                 2|         18000|\n",
      "|   Mahesh|null|      null| 40000|         28|                 5|         40000|\n",
      "|     null|  34|        10| 38000|         34|                10|         38000|\n",
      "|     null|  36|      null|  null|         36|                 5|         25750|\n",
      "+---------+----+----------+------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "impute.fit(df).transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName(\"DataFrm\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.csv(\"test.csv\",header=True,inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|  Sunny| 29|         4| 20000|\n",
      "|   Paul| 24|         3| 20000|\n",
      "| Harsha| 21|         1| 15000|\n",
      "|Shubham| 23|         2| 18000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Salary of the people less than 20000\n",
    "df.filter(\"Salary<=20000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   Name|age|\n",
      "+-------+---+\n",
      "|  Sunny| 29|\n",
      "|   Paul| 24|\n",
      "| Harsha| 21|\n",
      "|Shubham| 23|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Selecting columns with filters\n",
    "df.filter(\"Salary<=20000\").select([\"Name\",\"age\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2nd Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+\n",
      "|   Name|age|Salary|\n",
      "+-------+---+------+\n",
      "|  Sunny| 29| 20000|\n",
      "|   Paul| 24| 20000|\n",
      "| Harsha| 21| 15000|\n",
      "|Shubham| 23| 18000|\n",
      "+-------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df[\"Salary\"]<=20000).select([\"Name\",\"age\",\"Salary\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3rd way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(\"Salary<=20000\").orderBy(\"Name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting 2 conditions together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|  Sunny| 29|         4| 20000|\n",
      "|   Paul| 24|         3| 20000|\n",
      "|Shubham| 23|         2| 18000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df[\"Salary\"]<=20000) & (df[\"age\"]>=23)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INverse filter operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|  Sunny| 29|         4| 20000|\n",
      "|   Paul| 24|         3| 20000|\n",
      "| Harsha| 21|         1| 15000|\n",
      "|Shubham| 23|         2| 18000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(~(df[\"Salary\"]>20000)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName(\"Practise\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Departments: string, salary: int, Hours: int]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=spark.read.csv(\"test2.csv\",header=True,inferSchema=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+-----+\n",
      "|     Name| Departments|salary|Hours|\n",
      "+---------+------------+------+-----+\n",
      "|    Krish|Data Science| 10000|    5|\n",
      "|    Krish|         IOT|  5000|    3|\n",
      "|   Mahesh|    Big Data|  4000|    6|\n",
      "|    Krish|    Big Data|  4000|    2|\n",
      "|   Mahesh|Data Science|  3000|    5|\n",
      "|Sudhanshu|Data Science| 20000|    4|\n",
      "|Sudhanshu|         IOT| 10000|    8|\n",
      "|Sudhanshu|    Big Data|  5000|    4|\n",
      "|    Sunny|Data Science| 10000|    5|\n",
      "|    Sunny|    Big Data|  2000|    5|\n",
      "+---------+------------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Departments: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GroupBY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+----------+\n",
      "|     Name|sum(salary)|sum(Hours)|\n",
      "+---------+-----------+----------+\n",
      "|Sudhanshu|      35000|        16|\n",
      "|    Sunny|      12000|        10|\n",
      "|    Krish|      19000|        10|\n",
      "|   Mahesh|       7000|        11|\n",
      "+---------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby(\"Name\").sum().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by slected columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+\n",
      "| Departments|avg(Hours)|\n",
      "+------------+----------+\n",
      "|         IOT|       5.5|\n",
      "|    Big Data|      4.25|\n",
      "|Data Science|      4.75|\n",
      "+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## count num of depatment\n",
    "df.groupby(\"Departments\").avg(\"Hours\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying agg function directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(Salary)|\n",
      "+-----------+\n",
      "|      73000|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg({\"Salary\" : \"sum\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training -------------------------- ####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "|     null|  34|        10| 38000|\n",
      "|     null|  36|      null|  null|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.csv(\"test1.csv\",header=True,inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+-----+\n",
      "|     Name|age|Experience|Salary|Bonus|\n",
      "+---------+---+----------+------+-----+\n",
      "|     null| 36|      null|  null| null|\n",
      "|   Harsha| 21|         1| 15000|  500|\n",
      "|  Shubham| 23|         2| 18000| 1000|\n",
      "|     Paul| 24|         3| 20000| 1500|\n",
      "|    Sunny| 29|         4| 20000| 2000|\n",
      "|Sudhanshu| 30|         8| 25000| 4000|\n",
      "|    Krish| 31|        10| 30000| 5000|\n",
      "|     null| 34|        10| 38000| 5000|\n",
      "+---------+---+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"age\").isNotNull()).withColumn(\"Bonus\",(col(\"Experience\")*500).cast(\"int\")).sort(col(\"Bonus\").asc()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "| age|     naam|\n",
      "+----+---------+\n",
      "|  31|    Krish|\n",
      "|  30|Sudhanshu|\n",
      "|  29|    Sunny|\n",
      "|  24|     Paul|\n",
      "|  21|   Harsha|\n",
      "|  23|  Shubham|\n",
      "|null|   Mahesh|\n",
      "|  34|     null|\n",
      "|  36|     null|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nv=df.select(\"age\",col(\"Name\").alias(\"naam\"))\n",
    "nv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-----------+\n",
      "| age|     Name|Name_select|\n",
      "+----+---------+-----------+\n",
      "|  31|    Krish|       true|\n",
      "|  30|Sudhanshu|      false|\n",
      "|  29|    Sunny|       true|\n",
      "|  24|     Paul|      false|\n",
      "|  21|   Harsha|      false|\n",
      "|  23|  Shubham|      false|\n",
      "|null|   Mahesh|      false|\n",
      "|  34|     null|       null|\n",
      "|  36|     null|       null|\n",
      "+----+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nv=df.selectExpr(\"age\",\"Name\",\"Name in ('Krish','Sunny') as Name_select\")\n",
    "nv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     Name|\n",
      "+---------+\n",
      "|    Krish|\n",
      "|Sudhanshu|\n",
      "|    Sunny|\n",
      "|     Paul|\n",
      "|   Harsha|\n",
      "|  Shubham|\n",
      "|   Mahesh|\n",
      "|     null|\n",
      "|     null|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"Name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns = [\"language\",\"users_count\"]\n",
    "data = [(\"Java\", 20000), (\"Python\", 100000), (\"Scala\", 3000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|language|users_count|\n",
      "+--------+-----------+\n",
      "|    Java|      20000|\n",
      "|  Python|     100000|\n",
      "|   Scala|       3000|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame(data).toDF(*columns)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- language: string (nullable = true)\n",
      " |-- users_count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.createDataFrame([(\"a\",1),(\"b\",2),(\"c\",3)]).toDF(*[\"Name\",\"Age\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<map at 0x255eaecbb88>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rowData = map(lambda x: [i for i in x], data) \n",
    "rowData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd3 = spark.createDataFrame(rowData,columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[language: string, users_count: string, _3: string]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-------+\n",
      "|  language|users_count|     _3|\n",
      "+----------+-----------+-------+\n",
      "|student_id|       name|address|\n",
      "+----------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dd3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:SPARK_HOME]",
   "language": "python",
   "name": "conda-env-SPARK_HOME-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
